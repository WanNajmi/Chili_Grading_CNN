{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation,BatchNormalization, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.optimizers import *\n",
    "\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 426 files belonging to 6 classes.\n",
      "Using 341 files for training.\n"
     ]
    }
   ],
   "source": [
    "#load train images\n",
    "img_size=160\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory('C:/Users/User/Documents/UPM/Semester 3/Category/basedata',\n",
    "                                                            validation_split=0.2,\n",
    "                                                            batch_size = 32,\n",
    "                                                            label_mode = 'categorical',\n",
    "                                                            subset='training',\n",
    "                                                            seed=123,\n",
    "                                                            image_size=(img_size,img_size),\n",
    "                                                            shuffle=True\n",
    "                                                           )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 426 files belonging to 6 classes.\n",
      "Using 85 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#load test images\n",
    "img_size=160\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory('C:/Users/User/Desktop/Category/basedata/',\n",
    "                                                           validation_split=0.2,\n",
    "                                                           batch_size = 32,\n",
    "                                                           label_mode = 'categorical',\n",
    "                                                           subset='validation',\n",
    "                                                           seed=123,\n",
    "                                                           image_size=(img_size,img_size),\n",
    "                                                           shuffle=True\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 150, 150, 64)      23296     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 75, 75, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 75, 75, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 65, 65, 128)       991360    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 32, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 22, 22, 256)       3965184   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 11, 11, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 11, 11, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 256)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,049,990\n",
      "Trainable params: 5,048,582\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_size=160\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "# Can be omitted, you can specify the input_shape in other layers\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(img_size,img_size,3)))\n",
    "\n",
    "# 1st 2D Convolution layer\n",
    "model.add(tf.keras.layers.Conv2D(64, kernel_size=(11,11), activation='relu'))\n",
    "# Max Pool layer \n",
    "# It downsmaples the input representetion within the pool_size size\n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "# Normalization layer\n",
    "# The layer normalizes its output using the mean and standard deviation of the current batch of inputs.\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "# 2nd 2D Convolution layer\n",
    "model.add(tf.keras.layers.Conv2D(128, kernel_size=(11,11),activation='relu'))\n",
    "# Max Pool layer \n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "# Normalization layer\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# 1st fully convolutional layer \n",
    "model.add(tf.keras.layers.Conv2D(256, kernel_size=(11,11),activation='relu'))\n",
    "# Max Pool layer \n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "# Normalization layer\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "# Global Max Pool layer\n",
    "model.add(tf.keras.layers.GlobalMaxPool2D())\n",
    "\n",
    "\n",
    "# Dense Layers after flattening the data\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "# Normalization layer\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Add Output Layer\n",
    "model.add(tf.keras.layers.Dense(6, activation='softmax')) # = 12 predicted classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/270\n",
      "11/11 [==============================] - 18s 1s/step - loss: 1.5239 - accuracy: 0.4194 - val_loss: 348.5487 - val_accuracy: 0.1412\n",
      "Epoch 2/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.8362 - accuracy: 0.7009 - val_loss: 233.7131 - val_accuracy: 0.1412\n",
      "Epoch 3/270\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 0.6112 - accuracy: 0.7801 - val_loss: 147.4809 - val_accuracy: 0.1412\n",
      "Epoch 4/270\n",
      "11/11 [==============================] - 3s 222ms/step - loss: 0.4955 - accuracy: 0.7918 - val_loss: 87.8830 - val_accuracy: 0.1412\n",
      "Epoch 5/270\n",
      "11/11 [==============================] - 3s 216ms/step - loss: 0.4050 - accuracy: 0.8622 - val_loss: 78.0494 - val_accuracy: 0.1412\n",
      "Epoch 6/270\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 0.3383 - accuracy: 0.8768 - val_loss: 55.7255 - val_accuracy: 0.1412\n",
      "Epoch 7/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.2377 - accuracy: 0.9413 - val_loss: 43.5684 - val_accuracy: 0.1412\n",
      "Epoch 8/270\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 0.1820 - accuracy: 0.9619 - val_loss: 26.7055 - val_accuracy: 0.1412\n",
      "Epoch 9/270\n",
      "11/11 [==============================] - 3s 217ms/step - loss: 0.1671 - accuracy: 0.9531 - val_loss: 39.1486 - val_accuracy: 0.1412\n",
      "Epoch 10/270\n",
      "11/11 [==============================] - 3s 215ms/step - loss: 0.1266 - accuracy: 0.9765 - val_loss: 30.3327 - val_accuracy: 0.1412\n",
      "Epoch 11/270\n",
      "11/11 [==============================] - 3s 236ms/step - loss: 0.1021 - accuracy: 0.9853 - val_loss: 23.2184 - val_accuracy: 0.1412\n",
      "Epoch 12/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0643 - accuracy: 0.9941 - val_loss: 18.2847 - val_accuracy: 0.1412\n",
      "Epoch 13/270\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 0.0714 - accuracy: 0.9853 - val_loss: 13.2132 - val_accuracy: 0.1412\n",
      "Epoch 14/270\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 0.0894 - accuracy: 0.9824 - val_loss: 17.5396 - val_accuracy: 0.1412\n",
      "Epoch 15/270\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 0.0815 - accuracy: 0.9795 - val_loss: 11.0068 - val_accuracy: 0.1412\n",
      "Epoch 16/270\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 0.1321 - accuracy: 0.9589 - val_loss: 6.8203 - val_accuracy: 0.2824\n",
      "Epoch 17/270\n",
      "11/11 [==============================] - 3s 216ms/step - loss: 0.1438 - accuracy: 0.9384 - val_loss: 12.4850 - val_accuracy: 0.1412\n",
      "Epoch 18/270\n",
      "11/11 [==============================] - 3s 221ms/step - loss: 0.1268 - accuracy: 0.9677 - val_loss: 8.7264 - val_accuracy: 0.1529\n",
      "Epoch 19/270\n",
      "11/11 [==============================] - 3s 220ms/step - loss: 0.0991 - accuracy: 0.9765 - val_loss: 6.7128 - val_accuracy: 0.2235\n",
      "Epoch 20/270\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 0.0676 - accuracy: 0.9795 - val_loss: 6.9774 - val_accuracy: 0.1765\n",
      "Epoch 21/270\n",
      "11/11 [==============================] - 3s 216ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 6.9402 - val_accuracy: 0.1882\n",
      "Epoch 22/270\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 4.5653 - val_accuracy: 0.2706\n",
      "Epoch 23/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0193 - accuracy: 0.9971 - val_loss: 3.2497 - val_accuracy: 0.4235\n",
      "Epoch 24/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0181 - accuracy: 0.9971 - val_loss: 3.8782 - val_accuracy: 0.4000\n",
      "Epoch 25/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0376 - accuracy: 0.9883 - val_loss: 2.6096 - val_accuracy: 0.5176\n",
      "Epoch 26/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0350 - accuracy: 0.9883 - val_loss: 1.5080 - val_accuracy: 0.6353\n",
      "Epoch 27/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0553 - accuracy: 0.9795 - val_loss: 1.7252 - val_accuracy: 0.5647\n",
      "Epoch 28/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0834 - accuracy: 0.9677 - val_loss: 1.4960 - val_accuracy: 0.6824\n",
      "Epoch 29/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 0.0744 - accuracy: 0.9765 - val_loss: 6.2819 - val_accuracy: 0.2353\n",
      "Epoch 30/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0853 - accuracy: 0.9648 - val_loss: 6.4415 - val_accuracy: 0.2588\n",
      "Epoch 31/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0465 - accuracy: 0.9912 - val_loss: 3.7892 - val_accuracy: 0.3176\n",
      "Epoch 32/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0288 - accuracy: 0.9883 - val_loss: 2.5414 - val_accuracy: 0.4588\n",
      "Epoch 33/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 0.0221 - accuracy: 0.9971 - val_loss: 3.1719 - val_accuracy: 0.4118\n",
      "Epoch 34/270\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 0.0529 - accuracy: 0.9853 - val_loss: 5.3205 - val_accuracy: 0.3059\n",
      "Epoch 35/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0315 - accuracy: 0.9971 - val_loss: 4.4240 - val_accuracy: 0.3882\n",
      "Epoch 36/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0225 - accuracy: 0.9971 - val_loss: 4.0866 - val_accuracy: 0.4235\n",
      "Epoch 37/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 3.1545 - val_accuracy: 0.5294\n",
      "Epoch 38/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.6457 - val_accuracy: 0.5647\n",
      "Epoch 39/270\n",
      "11/11 [==============================] - 3s 217ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.2446 - val_accuracy: 0.6000\n",
      "Epoch 40/270\n",
      "11/11 [==============================] - 3s 206ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.5327 - val_accuracy: 0.5765\n",
      "Epoch 41/270\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.5060 - val_accuracy: 0.5529\n",
      "Epoch 42/270\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.0044 - val_accuracy: 0.7059\n",
      "Epoch 43/270\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.0655 - val_accuracy: 0.6353\n",
      "Epoch 44/270\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.4858 - val_accuracy: 0.5412\n",
      "Epoch 45/270\n",
      "11/11 [==============================] - 3s 220ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.2098 - val_accuracy: 0.6118\n",
      "Epoch 46/270\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.2655 - val_accuracy: 0.7176\n",
      "Epoch 47/270\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.2086 - val_accuracy: 0.7294\n",
      "Epoch 48/270\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.9880 - val_accuracy: 0.7176\n",
      "Epoch 49/270\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.9533 - val_accuracy: 0.7059\n",
      "Epoch 50/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.9779 - val_accuracy: 0.7059\n",
      "Epoch 51/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.9808 - val_accuracy: 0.7412\n",
      "Epoch 52/270\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 9.0801e-04 - accuracy: 1.0000 - val_loss: 1.0191 - val_accuracy: 0.7529\n",
      "Epoch 53/270\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.0746 - val_accuracy: 0.7412\n",
      "Epoch 54/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.0651 - val_accuracy: 0.7412\n",
      "Epoch 55/270\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1367 - val_accuracy: 0.7176\n",
      "Epoch 56/270\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.7729 - val_accuracy: 0.5765\n",
      "Epoch 57/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.2309 - val_accuracy: 0.7176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1852 - val_accuracy: 0.7412\n",
      "Epoch 59/270\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 9.1567e-04 - accuracy: 1.0000 - val_loss: 1.1999 - val_accuracy: 0.7412\n",
      "Epoch 60/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.2121 - val_accuracy: 0.7176\n",
      "Epoch 61/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 5.7803e-04 - accuracy: 1.0000 - val_loss: 1.2431 - val_accuracy: 0.7176\n",
      "Epoch 62/270\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 7.5241e-04 - accuracy: 1.0000 - val_loss: 1.2578 - val_accuracy: 0.7176\n",
      "Epoch 63/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 7.7485e-04 - accuracy: 1.0000 - val_loss: 1.2633 - val_accuracy: 0.7294\n",
      "Epoch 64/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 7.6529e-04 - accuracy: 1.0000 - val_loss: 1.2762 - val_accuracy: 0.7412\n",
      "Epoch 65/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 9.4057e-04 - accuracy: 1.0000 - val_loss: 1.3152 - val_accuracy: 0.7412\n",
      "Epoch 66/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 6.1637e-04 - accuracy: 1.0000 - val_loss: 1.2927 - val_accuracy: 0.7412\n",
      "Epoch 67/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 7.8752e-04 - accuracy: 1.0000 - val_loss: 1.2888 - val_accuracy: 0.7529\n",
      "Epoch 68/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 5.3691e-04 - accuracy: 1.0000 - val_loss: 1.2930 - val_accuracy: 0.7294\n",
      "Epoch 69/270\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.2968 - val_accuracy: 0.7176\n",
      "Epoch 70/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 5.5272e-04 - accuracy: 1.0000 - val_loss: 1.3122 - val_accuracy: 0.7176\n",
      "Epoch 71/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 5.3034e-04 - accuracy: 1.0000 - val_loss: 1.3248 - val_accuracy: 0.7176\n",
      "Epoch 72/270\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 7.4954e-04 - accuracy: 1.0000 - val_loss: 1.3356 - val_accuracy: 0.7294\n",
      "Epoch 73/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.3641 - val_accuracy: 0.7412\n",
      "Epoch 74/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 9.5845e-04 - accuracy: 1.0000 - val_loss: 1.3676 - val_accuracy: 0.7059\n",
      "Epoch 75/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 6.2026e-04 - accuracy: 1.0000 - val_loss: 1.3685 - val_accuracy: 0.7294\n",
      "Epoch 76/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 6.3867e-04 - accuracy: 1.0000 - val_loss: 1.3876 - val_accuracy: 0.7294\n",
      "Epoch 77/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 6.1811e-04 - accuracy: 1.0000 - val_loss: 1.3931 - val_accuracy: 0.7176\n",
      "Epoch 78/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.4007 - val_accuracy: 0.7059\n",
      "Epoch 79/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 5.1317e-04 - accuracy: 1.0000 - val_loss: 1.4715 - val_accuracy: 0.7294\n",
      "Epoch 80/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 7.3121e-04 - accuracy: 1.0000 - val_loss: 1.4704 - val_accuracy: 0.7294\n",
      "Epoch 81/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.5088 - val_accuracy: 0.6941\n",
      "Epoch 82/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 9.1661e-04 - accuracy: 1.0000 - val_loss: 1.4979 - val_accuracy: 0.7059\n",
      "Epoch 83/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 5.0541e-04 - accuracy: 1.0000 - val_loss: 1.4864 - val_accuracy: 0.7176\n",
      "Epoch 84/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.9295e-04 - accuracy: 1.0000 - val_loss: 1.5051 - val_accuracy: 0.7412\n",
      "Epoch 85/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 5.8199e-04 - accuracy: 1.0000 - val_loss: 1.5045 - val_accuracy: 0.7412\n",
      "Epoch 86/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1290e-04 - accuracy: 1.0000 - val_loss: 1.4691 - val_accuracy: 0.7412\n",
      "Epoch 87/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 3.6729e-04 - accuracy: 1.0000 - val_loss: 1.4554 - val_accuracy: 0.7294\n",
      "Epoch 88/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 6.0666e-04 - accuracy: 1.0000 - val_loss: 1.4494 - val_accuracy: 0.7294\n",
      "Epoch 89/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1090e-04 - accuracy: 1.0000 - val_loss: 1.4616 - val_accuracy: 0.7176\n",
      "Epoch 90/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 3.1726e-04 - accuracy: 1.0000 - val_loss: 1.4652 - val_accuracy: 0.7059\n",
      "Epoch 91/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.5223e-04 - accuracy: 1.0000 - val_loss: 1.4731 - val_accuracy: 0.7176\n",
      "Epoch 92/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 3.2533e-04 - accuracy: 1.0000 - val_loss: 1.4746 - val_accuracy: 0.7294\n",
      "Epoch 93/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.2119e-04 - accuracy: 1.0000 - val_loss: 1.4817 - val_accuracy: 0.7412\n",
      "Epoch 94/270\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 3.5674e-04 - accuracy: 1.0000 - val_loss: 1.4833 - val_accuracy: 0.7412\n",
      "Epoch 95/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 3.3178e-04 - accuracy: 1.0000 - val_loss: 1.4900 - val_accuracy: 0.7529\n",
      "Epoch 96/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.6459e-04 - accuracy: 1.0000 - val_loss: 1.5034 - val_accuracy: 0.7294\n",
      "Epoch 97/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 2.9379e-04 - accuracy: 1.0000 - val_loss: 1.5139 - val_accuracy: 0.7176\n",
      "Epoch 98/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 3.4062e-04 - accuracy: 1.0000 - val_loss: 1.5080 - val_accuracy: 0.7176\n",
      "Epoch 99/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 6.1177e-04 - accuracy: 1.0000 - val_loss: 1.4860 - val_accuracy: 0.7294\n",
      "Epoch 100/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 3.5424e-04 - accuracy: 1.0000 - val_loss: 1.4802 - val_accuracy: 0.7176\n",
      "Epoch 101/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 3.5184e-04 - accuracy: 1.0000 - val_loss: 1.4898 - val_accuracy: 0.7294\n",
      "Epoch 102/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 3.6356e-04 - accuracy: 1.0000 - val_loss: 1.4963 - val_accuracy: 0.7176\n",
      "Epoch 103/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 3.1621e-04 - accuracy: 1.0000 - val_loss: 1.4968 - val_accuracy: 0.7176\n",
      "Epoch 104/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.0710e-04 - accuracy: 1.0000 - val_loss: 1.4976 - val_accuracy: 0.7412\n",
      "Epoch 105/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 3.9854e-04 - accuracy: 1.0000 - val_loss: 1.4966 - val_accuracy: 0.7412\n",
      "Epoch 106/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 2.8685e-04 - accuracy: 1.0000 - val_loss: 1.4920 - val_accuracy: 0.7412\n",
      "Epoch 107/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 2.0295e-04 - accuracy: 1.0000 - val_loss: 1.4883 - val_accuracy: 0.7412\n",
      "Epoch 108/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 2.0121e-04 - accuracy: 1.0000 - val_loss: 1.4890 - val_accuracy: 0.7412\n",
      "Epoch 109/270\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 3.6509e-04 - accuracy: 1.0000 - val_loss: 1.4768 - val_accuracy: 0.7529\n",
      "Epoch 110/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 2.0734e-04 - accuracy: 1.0000 - val_loss: 1.4780 - val_accuracy: 0.7529\n",
      "Epoch 111/270\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 2.3324e-04 - accuracy: 1.0000 - val_loss: 1.4908 - val_accuracy: 0.7529\n",
      "Epoch 112/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.3097e-04 - accuracy: 1.0000 - val_loss: 1.4882 - val_accuracy: 0.7529\n",
      "Epoch 113/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 3.5366e-04 - accuracy: 1.0000 - val_loss: 2.1485 - val_accuracy: 0.5294\n",
      "Epoch 114/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.6590e-04 - accuracy: 1.0000 - val_loss: 1.5346 - val_accuracy: 0.7647\n",
      "Epoch 115/270\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 3.8150e-04 - accuracy: 1.0000 - val_loss: 1.5289 - val_accuracy: 0.7529\n",
      "Epoch 116/270\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 3.5682e-04 - accuracy: 1.0000 - val_loss: 1.5674 - val_accuracy: 0.7176\n",
      "Epoch 117/270\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 3.7938e-04 - accuracy: 1.0000 - val_loss: 1.5569 - val_accuracy: 0.7412\n",
      "Epoch 118/270\n",
      "11/11 [==============================] - 3s 224ms/step - loss: 1.8364e-04 - accuracy: 1.0000 - val_loss: 1.5310 - val_accuracy: 0.7412\n",
      "Epoch 119/270\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 1.8293e-04 - accuracy: 1.0000 - val_loss: 1.5068 - val_accuracy: 0.7529\n",
      "Epoch 120/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 2.8915e-04 - accuracy: 1.0000 - val_loss: 1.4939 - val_accuracy: 0.7412\n",
      "Epoch 121/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 3.2055e-04 - accuracy: 1.0000 - val_loss: 1.4952 - val_accuracy: 0.7529\n",
      "Epoch 122/270\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 2.4062e-04 - accuracy: 1.0000 - val_loss: 1.4892 - val_accuracy: 0.7412\n",
      "Epoch 123/270\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 2.0119e-04 - accuracy: 1.0000 - val_loss: 1.4884 - val_accuracy: 0.7529\n",
      "Epoch 124/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 2.1800e-04 - accuracy: 1.0000 - val_loss: 1.4828 - val_accuracy: 0.7412\n",
      "Epoch 125/270\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 2.1238e-04 - accuracy: 1.0000 - val_loss: 1.4940 - val_accuracy: 0.7412\n",
      "Epoch 126/270\n",
      "11/11 [==============================] - 3s 214ms/step - loss: 3.2527e-04 - accuracy: 1.0000 - val_loss: 1.5265 - val_accuracy: 0.7294\n",
      "Epoch 127/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 3.8798e-04 - accuracy: 1.0000 - val_loss: 1.5352 - val_accuracy: 0.7412\n",
      "Epoch 128/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 3.0963e-04 - accuracy: 1.0000 - val_loss: 1.5453 - val_accuracy: 0.7412\n",
      "Epoch 129/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.2528e-04 - accuracy: 1.0000 - val_loss: 1.5682 - val_accuracy: 0.7294\n",
      "Epoch 130/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 1.8779e-04 - accuracy: 1.0000 - val_loss: 1.5519 - val_accuracy: 0.7294\n",
      "Epoch 131/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.3072e-04 - accuracy: 1.0000 - val_loss: 1.5531 - val_accuracy: 0.7412\n",
      "Epoch 132/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.4615e-04 - accuracy: 1.0000 - val_loss: 1.5910 - val_accuracy: 0.7412\n",
      "Epoch 133/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 2.2020e-04 - accuracy: 1.0000 - val_loss: 1.6140 - val_accuracy: 0.7294\n",
      "Epoch 134/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 1.8718e-04 - accuracy: 1.0000 - val_loss: 1.6196 - val_accuracy: 0.7412\n",
      "Epoch 135/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.7710e-04 - accuracy: 1.0000 - val_loss: 1.5998 - val_accuracy: 0.7529\n",
      "Epoch 136/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.1719e-04 - accuracy: 1.0000 - val_loss: 1.6006 - val_accuracy: 0.7412\n",
      "Epoch 137/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 1.5316e-04 - accuracy: 1.0000 - val_loss: 1.5945 - val_accuracy: 0.7412\n",
      "Epoch 138/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.3817e-04 - accuracy: 1.0000 - val_loss: 1.5895 - val_accuracy: 0.7529\n",
      "Epoch 139/270\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 1.6479e-04 - accuracy: 1.0000 - val_loss: 1.5852 - val_accuracy: 0.7529\n",
      "Epoch 140/270\n",
      "11/11 [==============================] - 3s 226ms/step - loss: 2.4248e-04 - accuracy: 1.0000 - val_loss: 1.5759 - val_accuracy: 0.7529\n",
      "Epoch 141/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 2.8223e-04 - accuracy: 1.0000 - val_loss: 1.5748 - val_accuracy: 0.7529\n",
      "Epoch 142/270\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 1.3194e-04 - accuracy: 1.0000 - val_loss: 1.5751 - val_accuracy: 0.7529\n",
      "Epoch 143/270\n",
      "11/11 [==============================] - 3s 218ms/step - loss: 1.5051e-04 - accuracy: 1.0000 - val_loss: 1.5691 - val_accuracy: 0.7529\n",
      "Epoch 144/270\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 2.6452e-04 - accuracy: 1.0000 - val_loss: 1.5730 - val_accuracy: 0.7412\n",
      "Epoch 145/270\n",
      "11/11 [==============================] - 3s 218ms/step - loss: 2.0093e-04 - accuracy: 1.0000 - val_loss: 1.5772 - val_accuracy: 0.7412\n",
      "Epoch 146/270\n",
      "11/11 [==============================] - 3s 216ms/step - loss: 1.3483e-04 - accuracy: 1.0000 - val_loss: 1.5705 - val_accuracy: 0.7529\n",
      "Epoch 147/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 2.4023e-04 - accuracy: 1.0000 - val_loss: 1.5598 - val_accuracy: 0.7529\n",
      "Epoch 148/270\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 1.2236e-04 - accuracy: 1.0000 - val_loss: 1.5542 - val_accuracy: 0.7529\n",
      "Epoch 149/270\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 2.1623e-04 - accuracy: 1.0000 - val_loss: 1.5692 - val_accuracy: 0.7412\n",
      "Epoch 150/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 1.8774e-04 - accuracy: 1.0000 - val_loss: 1.5772 - val_accuracy: 0.7412\n",
      "Epoch 151/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 2.2777e-04 - accuracy: 1.0000 - val_loss: 1.5641 - val_accuracy: 0.7412\n",
      "Epoch 152/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.5458e-04 - accuracy: 1.0000 - val_loss: 1.5555 - val_accuracy: 0.7529\n",
      "Epoch 153/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.8614e-04 - accuracy: 1.0000 - val_loss: 1.5602 - val_accuracy: 0.7529\n",
      "Epoch 154/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.8509e-04 - accuracy: 1.0000 - val_loss: 1.5678 - val_accuracy: 0.7529\n",
      "Epoch 155/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 3.0838e-04 - accuracy: 1.0000 - val_loss: 1.6038 - val_accuracy: 0.7412\n",
      "Epoch 156/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.5098e-04 - accuracy: 1.0000 - val_loss: 1.5979 - val_accuracy: 0.7059\n",
      "Epoch 157/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.9308e-04 - accuracy: 1.0000 - val_loss: 1.5835 - val_accuracy: 0.7176\n",
      "Epoch 158/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.1730e-04 - accuracy: 1.0000 - val_loss: 1.5645 - val_accuracy: 0.7294\n",
      "Epoch 159/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.2086e-04 - accuracy: 1.0000 - val_loss: 1.5627 - val_accuracy: 0.7412\n",
      "Epoch 160/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 2.3905e-04 - accuracy: 1.0000 - val_loss: 1.5841 - val_accuracy: 0.7412\n",
      "Epoch 161/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 1.6496e-04 - accuracy: 1.0000 - val_loss: 1.5980 - val_accuracy: 0.7412\n",
      "Epoch 162/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.5690e-04 - accuracy: 1.0000 - val_loss: 1.6256 - val_accuracy: 0.7412\n",
      "Epoch 163/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.5208e-04 - accuracy: 1.0000 - val_loss: 1.6288 - val_accuracy: 0.7529\n",
      "Epoch 164/270\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 1.6230e-04 - accuracy: 1.0000 - val_loss: 1.6316 - val_accuracy: 0.7412\n",
      "Epoch 165/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 1.8935e-04 - accuracy: 1.0000 - val_loss: 1.5924 - val_accuracy: 0.7412\n",
      "Epoch 166/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 1.6009e-04 - accuracy: 1.0000 - val_loss: 1.5849 - val_accuracy: 0.7412\n",
      "Epoch 167/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 2.5212e-04 - accuracy: 1.0000 - val_loss: 1.6107 - val_accuracy: 0.7529\n",
      "Epoch 168/270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 194ms/step - loss: 1.9527e-04 - accuracy: 1.0000 - val_loss: 1.5821 - val_accuracy: 0.7412\n",
      "Epoch 169/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 2.1308e-04 - accuracy: 1.0000 - val_loss: 1.5857 - val_accuracy: 0.7294\n",
      "Epoch 170/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.2171e-04 - accuracy: 1.0000 - val_loss: 1.6462 - val_accuracy: 0.7294\n",
      "Epoch 171/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 3.4521e-04 - accuracy: 1.0000 - val_loss: 1.6337 - val_accuracy: 0.7294\n",
      "Epoch 172/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 2.1075e-04 - accuracy: 1.0000 - val_loss: 1.5888 - val_accuracy: 0.7412\n",
      "Epoch 173/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 2.0391e-04 - accuracy: 1.0000 - val_loss: 1.5601 - val_accuracy: 0.7412\n",
      "Epoch 174/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.4014e-04 - accuracy: 1.0000 - val_loss: 1.5851 - val_accuracy: 0.7294\n",
      "Epoch 175/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.3012e-04 - accuracy: 1.0000 - val_loss: 1.5855 - val_accuracy: 0.7294\n",
      "Epoch 176/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.1977e-04 - accuracy: 1.0000 - val_loss: 1.5778 - val_accuracy: 0.7294\n",
      "Epoch 177/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.2559e-04 - accuracy: 1.0000 - val_loss: 1.5876 - val_accuracy: 0.7294\n",
      "Epoch 178/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.1704e-04 - accuracy: 1.0000 - val_loss: 1.5686 - val_accuracy: 0.7294\n",
      "Epoch 179/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.0398e-04 - accuracy: 1.0000 - val_loss: 1.5674 - val_accuracy: 0.7294\n",
      "Epoch 180/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 1.2736e-04 - accuracy: 1.0000 - val_loss: 1.5655 - val_accuracy: 0.7412\n",
      "Epoch 181/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 1.2534e-04 - accuracy: 1.0000 - val_loss: 1.5732 - val_accuracy: 0.7529\n",
      "Epoch 182/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 8.8993e-05 - accuracy: 1.0000 - val_loss: 1.5803 - val_accuracy: 0.7529\n",
      "Epoch 183/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.3595e-04 - accuracy: 1.0000 - val_loss: 1.5838 - val_accuracy: 0.7529\n",
      "Epoch 184/270\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 1.0780e-04 - accuracy: 1.0000 - val_loss: 1.5859 - val_accuracy: 0.7529\n",
      "Epoch 185/270\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 9.8023e-05 - accuracy: 1.0000 - val_loss: 1.5958 - val_accuracy: 0.7529\n",
      "Epoch 186/270\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 1.3498e-04 - accuracy: 1.0000 - val_loss: 1.5995 - val_accuracy: 0.7529\n",
      "Epoch 187/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 7.9055e-05 - accuracy: 1.0000 - val_loss: 1.6001 - val_accuracy: 0.7529\n",
      "Epoch 188/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 8.8916e-05 - accuracy: 1.0000 - val_loss: 1.6046 - val_accuracy: 0.7529\n",
      "Epoch 189/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 8.2654e-05 - accuracy: 1.0000 - val_loss: 1.6152 - val_accuracy: 0.7529\n",
      "Epoch 190/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 1.3057e-04 - accuracy: 1.0000 - val_loss: 1.6250 - val_accuracy: 0.7529\n",
      "Epoch 191/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 1.0825e-04 - accuracy: 1.0000 - val_loss: 1.6542 - val_accuracy: 0.7412\n",
      "Epoch 192/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 8.9085e-05 - accuracy: 1.0000 - val_loss: 1.6526 - val_accuracy: 0.7412\n",
      "Epoch 193/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 2.4677e-04 - accuracy: 1.0000 - val_loss: 1.6554 - val_accuracy: 0.7412\n",
      "Epoch 194/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 1.4227e-04 - accuracy: 1.0000 - val_loss: 1.6413 - val_accuracy: 0.7529\n",
      "Epoch 195/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 1.1699e-04 - accuracy: 1.0000 - val_loss: 1.6305 - val_accuracy: 0.7529\n",
      "Epoch 196/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 2.0493e-04 - accuracy: 1.0000 - val_loss: 1.5982 - val_accuracy: 0.7529\n",
      "Epoch 197/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 1.4619e-04 - accuracy: 1.0000 - val_loss: 1.8039 - val_accuracy: 0.6588\n",
      "Epoch 198/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 4.4558e-04 - accuracy: 1.0000 - val_loss: 1.8336 - val_accuracy: 0.7059\n",
      "Epoch 199/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 1.2964e-04 - accuracy: 1.0000 - val_loss: 1.9410 - val_accuracy: 0.7059\n",
      "Epoch 200/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.2895 - val_accuracy: 0.7059\n",
      "Epoch 201/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.1436 - val_accuracy: 0.5765\n",
      "Epoch 202/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 6.8610e-04 - accuracy: 1.0000 - val_loss: 3.1011 - val_accuracy: 0.6118\n",
      "Epoch 203/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 4.6191e-04 - accuracy: 1.0000 - val_loss: 2.6268 - val_accuracy: 0.6824\n",
      "Epoch 204/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 8.0180e-04 - accuracy: 1.0000 - val_loss: 1.6220 - val_accuracy: 0.7176\n",
      "Epoch 205/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 5.5613e-04 - accuracy: 1.0000 - val_loss: 1.6133 - val_accuracy: 0.7294\n",
      "Epoch 206/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 3.8009e-04 - accuracy: 1.0000 - val_loss: 1.6743 - val_accuracy: 0.6941\n",
      "Epoch 207/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 3.0870e-04 - accuracy: 1.0000 - val_loss: 1.6593 - val_accuracy: 0.7059\n",
      "Epoch 208/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 0.0045 - accuracy: 0.9971 - val_loss: 6.0085 - val_accuracy: 0.3529\n",
      "Epoch 209/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.4981 - accuracy: 0.8651 - val_loss: 14.4818 - val_accuracy: 0.2000\n",
      "Epoch 210/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.9817 - accuracy: 0.6422 - val_loss: 87.1525 - val_accuracy: 0.1176\n",
      "Epoch 211/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.7597 - accuracy: 0.7097 - val_loss: 58.7775 - val_accuracy: 0.1529\n",
      "Epoch 212/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.5147 - accuracy: 0.7918 - val_loss: 86.5868 - val_accuracy: 0.1412\n",
      "Epoch 213/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.4099 - accuracy: 0.8387 - val_loss: 48.6094 - val_accuracy: 0.1412\n",
      "Epoch 214/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.3321 - accuracy: 0.8680 - val_loss: 47.2944 - val_accuracy: 0.1412\n",
      "Epoch 215/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.2033 - accuracy: 0.9413 - val_loss: 35.3559 - val_accuracy: 0.1412\n",
      "Epoch 216/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.1374 - accuracy: 0.9560 - val_loss: 28.9778 - val_accuracy: 0.1412\n",
      "Epoch 217/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.1014 - accuracy: 0.9765 - val_loss: 21.5767 - val_accuracy: 0.1412\n",
      "Epoch 218/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0673 - accuracy: 0.9824 - val_loss: 11.9584 - val_accuracy: 0.1882\n",
      "Epoch 219/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.1304 - accuracy: 0.9501 - val_loss: 8.8227 - val_accuracy: 0.3294\n",
      "Epoch 220/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.1042 - accuracy: 0.9501 - val_loss: 13.0389 - val_accuracy: 0.1412\n",
      "Epoch 221/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.1100 - accuracy: 0.9619 - val_loss: 6.9084 - val_accuracy: 0.2588\n",
      "Epoch 222/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.1229 - accuracy: 0.9531 - val_loss: 9.2984 - val_accuracy: 0.2588\n",
      "Epoch 223/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.1210 - accuracy: 0.9795 - val_loss: 11.2984 - val_accuracy: 0.2118\n",
      "Epoch 224/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0441 - accuracy: 0.9853 - val_loss: 12.0122 - val_accuracy: 0.2118\n",
      "Epoch 225/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0227 - accuracy: 0.9971 - val_loss: 6.1538 - val_accuracy: 0.3529\n",
      "Epoch 226/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 2.4268 - val_accuracy: 0.5059\n",
      "Epoch 227/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 1.8856 - val_accuracy: 0.5647\n",
      "Epoch 228/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.8328 - val_accuracy: 0.5647\n",
      "Epoch 229/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.6967 - val_accuracy: 0.6118\n",
      "Epoch 230/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.5321 - val_accuracy: 0.6118\n",
      "Epoch 231/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.4549 - val_accuracy: 0.6353\n",
      "Epoch 232/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.6213 - val_accuracy: 0.6118\n",
      "Epoch 233/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.4450 - val_accuracy: 0.6353\n",
      "Epoch 234/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3563 - val_accuracy: 0.6588\n",
      "Epoch 235/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.2760 - val_accuracy: 0.6471\n",
      "Epoch 236/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.2213 - val_accuracy: 0.6588\n",
      "Epoch 237/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1332 - val_accuracy: 0.6706\n",
      "Epoch 238/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.1522 - val_accuracy: 0.7294\n",
      "Epoch 239/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.1448 - val_accuracy: 0.7412\n",
      "Epoch 240/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.1925 - val_accuracy: 0.7412\n",
      "Epoch 241/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.2696 - val_accuracy: 0.7294\n",
      "Epoch 242/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.2515 - val_accuracy: 0.7412\n",
      "Epoch 243/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.2377 - val_accuracy: 0.7412\n",
      "Epoch 244/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.2135 - val_accuracy: 0.7529\n",
      "Epoch 245/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.1856 - val_accuracy: 0.7647\n",
      "Epoch 246/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.1512 - val_accuracy: 0.7529\n",
      "Epoch 247/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.1326 - val_accuracy: 0.7412\n",
      "Epoch 248/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1312 - val_accuracy: 0.7294\n",
      "Epoch 249/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.1210 - val_accuracy: 0.7294\n",
      "Epoch 250/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.1226 - val_accuracy: 0.7176\n",
      "Epoch 251/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.1647 - val_accuracy: 0.7176\n",
      "Epoch 252/270\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.1386 - val_accuracy: 0.7412\n",
      "Epoch 253/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1264 - val_accuracy: 0.7294\n",
      "Epoch 254/270\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.1242 - val_accuracy: 0.7294\n",
      "Epoch 255/270\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 7.9291e-04 - accuracy: 1.0000 - val_loss: 1.1742 - val_accuracy: 0.7176\n",
      "Epoch 256/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.1641 - val_accuracy: 0.7176\n",
      "Epoch 257/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1819 - val_accuracy: 0.7412\n",
      "Epoch 258/270\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.2455 - val_accuracy: 0.7412\n",
      "Epoch 259/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 6.7930e-04 - accuracy: 1.0000 - val_loss: 1.2308 - val_accuracy: 0.7529\n",
      "Epoch 260/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 6.6486e-04 - accuracy: 1.0000 - val_loss: 1.2089 - val_accuracy: 0.7412\n",
      "Epoch 261/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 7.9794e-04 - accuracy: 1.0000 - val_loss: 1.1927 - val_accuracy: 0.7294\n",
      "Epoch 262/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 6.9448e-04 - accuracy: 1.0000 - val_loss: 1.1949 - val_accuracy: 0.7294\n",
      "Epoch 263/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 6.4547e-04 - accuracy: 1.0000 - val_loss: 1.1922 - val_accuracy: 0.7294\n",
      "Epoch 264/270\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 7.4144e-04 - accuracy: 1.0000 - val_loss: 1.1853 - val_accuracy: 0.7294\n",
      "Epoch 265/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 6.6709e-04 - accuracy: 1.0000 - val_loss: 1.1839 - val_accuracy: 0.7059\n",
      "Epoch 266/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 6.3324e-04 - accuracy: 1.0000 - val_loss: 1.1821 - val_accuracy: 0.7176\n",
      "Epoch 267/270\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 7.9240e-04 - accuracy: 1.0000 - val_loss: 1.1859 - val_accuracy: 0.7176\n",
      "Epoch 268/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 7.7314e-04 - accuracy: 1.0000 - val_loss: 1.1876 - val_accuracy: 0.7176\n",
      "Epoch 269/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 5.2924e-04 - accuracy: 1.0000 - val_loss: 1.1798 - val_accuracy: 0.7176\n",
      "Epoch 270/270\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.1811 - val_accuracy: 0.7176\n"
     ]
    }
   ],
   "source": [
    "epochs =270\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=epochs,\n",
    "    validation_data=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 34ms/step - loss: 0.0733 - categorical_accuracy: 0.7176\n",
      "Test Accuracy: 71.76%\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 0s - loss: 0.0733 - categorical_accuracy: 0.7176 - 177ms/epoch - 59ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 0s - loss: 1.1811 - accuracy: 0.7176 - 439ms/epoch - 146ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 39ms/step - loss: 2.7500e-04 - accuracy: 1.0000\n",
      "Train Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(train_dataset)\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Not Ripe', 'A Ripe', 'B Not Ripe', 'B Ripe', 'C Not Ripe', 'C Ripe']\n"
     ]
    }
   ],
   "source": [
    "class_names = test_dataset.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 23.07 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 A Ripe\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR10_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.12 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR11_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.06 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR12_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 22.96 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR13_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR14_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR15_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR16_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 31.36 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR1_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR2_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR3_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 34.91 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR4_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.15 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR5_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 34.71 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR6_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 27.47 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR77_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 32.10 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR78_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR7_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR7_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR8_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 34.87 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR9_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.20 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 A Not Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR10_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR11_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR12_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 33.69 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR13_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR14_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR15_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR16_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR1_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.19 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR2_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR3_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 23.02 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR4_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 34.04 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR5_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR6_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 34.87 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR79_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR7_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR80_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.17 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR81_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 34.97 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR8_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR9_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 23.58 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 B Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR10_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.12 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR11_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 21.70 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR12_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 25.46 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR13_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 34.32 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR14_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 23.64 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR15_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 33.98 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR16_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 27.79 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR1_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 34.26 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR2_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 32.55 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR3_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 23.17 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR4_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 32.53 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR5_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR65_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 33.26 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR66_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 35.15 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR67_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 28.58 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR6_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 33.17 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR7_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 24.95 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR8_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 30.67 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR9_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 B Not Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR10_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.09 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR11_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 32.71 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR12_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 34.51 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR13_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR14_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 20.94 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR15_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 32.04 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR16_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR1_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR2_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 26.33 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR3_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR4_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 34.48 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR5_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 25.91 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR69_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 34.61 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR6_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 20.91 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR70_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR71_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR7_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 32.58 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR8_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.11 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR9_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 C Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR11_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 32.41 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR13_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR22_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR24_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR30_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR32_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR38_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 31.83 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR44_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR45_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR46_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR47_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR4_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR50_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR51_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR60_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR62_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR63_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR64_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR8_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.20 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 C Not Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR10_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.19 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR11_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.20 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR1_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/User/Desktop/Category/test/C Not Ripe/CCNR20_0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-4b1c891b128d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrootdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/User/Desktop/Category/test/C Not Ripe/CCNR20_0.png\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m img = tf.keras.utils.load_img(\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mrootdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    311\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m   \"\"\"\n\u001b[1;32m--> 313\u001b[1;33m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[0;32m    314\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/User/Desktop/Category/test/C Not Ripe/CCNR20_0.png'"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CCNR20_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.18 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR22_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR28_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR30_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR38_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 34.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR43_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR52_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR57_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 22.35 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR58_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR59_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR5_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR62_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR63_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR64_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR6_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 26.72 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR9_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training')\n",
    "plt.plot(epochs_range, val_acc, label='Test')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training')\n",
    "plt.plot(epochs_range, val_loss, label='Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
