{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation,BatchNormalization, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.optimizers import *\n",
    "\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 426 files belonging to 6 classes.\n",
      "Using 299 files for training.\n"
     ]
    }
   ],
   "source": [
    "#load train images\n",
    "img_size=160\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory('C:/Users/User/Documents/UPM/Semester 3/Category/basedata',\n",
    "                                                            validation_split=0.3,\n",
    "                                                            batch_size = 32,\n",
    "                                                            label_mode = 'categorical',\n",
    "                                                            subset='training',\n",
    "                                                            seed=123,\n",
    "                                                            image_size=(img_size,img_size),\n",
    "                                                            shuffle=True\n",
    "                                                           )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 426 files belonging to 6 classes.\n",
      "Using 127 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#load test images\n",
    "img_size=160\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory('C:/Users/User/Desktop/Category/basedata/',\n",
    "                                                           validation_split=0.3,\n",
    "                                                           batch_size = 32,\n",
    "                                                           label_mode = 'categorical',\n",
    "                                                           subset='validation',\n",
    "                                                           seed=123,\n",
    "                                                           image_size=(img_size,img_size),\n",
    "                                                           shuffle=True\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 150, 150, 64)      23296     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 75, 75, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 75, 75, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 65, 65, 128)       991360    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 32, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 22, 22, 256)       3965184   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 11, 11, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 11, 11, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 256)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,049,990\n",
      "Trainable params: 5,048,582\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_size=160\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "# Can be omitted, you can specify the input_shape in other layers\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(img_size,img_size,3)))\n",
    "\n",
    "# 1st 2D Convolution layer\n",
    "model.add(tf.keras.layers.Conv2D(64, kernel_size=(11,11), activation='relu'))\n",
    "# Max Pool layer \n",
    "# It downsmaples the input representetion within the pool_size size\n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "# Normalization layer\n",
    "# The layer normalizes its output using the mean and standard deviation of the current batch of inputs.\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "# 2nd 2D Convolution layer\n",
    "model.add(tf.keras.layers.Conv2D(128, kernel_size=(11,11),activation='relu'))\n",
    "# Max Pool layer \n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "# Normalization layer\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# 1st fully convolutional layer \n",
    "model.add(tf.keras.layers.Conv2D(256, kernel_size=(11,11),activation='relu'))\n",
    "# Max Pool layer \n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "# Normalization layer\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "# Global Max Pool layer\n",
    "model.add(tf.keras.layers.GlobalMaxPool2D())\n",
    "\n",
    "\n",
    "# Dense Layers after flattening the data\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "# Normalization layer\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Add Output Layer\n",
    "model.add(tf.keras.layers.Dense(6, activation='softmax')) # = 12 predicted classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/270\n",
      "10/10 [==============================] - 17s 1s/step - loss: 1.6304 - accuracy: 0.4448 - val_loss: 664.1993 - val_accuracy: 0.1260\n",
      "Epoch 2/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.9071 - accuracy: 0.6154 - val_loss: 435.9639 - val_accuracy: 0.1260\n",
      "Epoch 3/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.6782 - accuracy: 0.7124 - val_loss: 267.0940 - val_accuracy: 0.1260\n",
      "Epoch 4/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.5642 - accuracy: 0.7625 - val_loss: 155.0402 - val_accuracy: 0.1260\n",
      "Epoch 5/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.4815 - accuracy: 0.8428 - val_loss: 108.6137 - val_accuracy: 0.1260\n",
      "Epoch 6/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.3789 - accuracy: 0.8696 - val_loss: 70.6673 - val_accuracy: 0.1260\n",
      "Epoch 7/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.3617 - accuracy: 0.8930 - val_loss: 52.7297 - val_accuracy: 0.1260\n",
      "Epoch 8/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.3292 - accuracy: 0.9030 - val_loss: 45.2603 - val_accuracy: 0.1260\n",
      "Epoch 9/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.3118 - accuracy: 0.8963 - val_loss: 43.7230 - val_accuracy: 0.1260\n",
      "Epoch 10/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.2628 - accuracy: 0.9231 - val_loss: 31.8864 - val_accuracy: 0.1260\n",
      "Epoch 11/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.2844 - accuracy: 0.8930 - val_loss: 18.8032 - val_accuracy: 0.1260\n",
      "Epoch 12/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1943 - accuracy: 0.9532 - val_loss: 19.7325 - val_accuracy: 0.1260\n",
      "Epoch 13/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1487 - accuracy: 0.9632 - val_loss: 19.5959 - val_accuracy: 0.1260\n",
      "Epoch 14/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.1359 - accuracy: 0.9666 - val_loss: 12.8243 - val_accuracy: 0.1575\n",
      "Epoch 15/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1374 - accuracy: 0.9699 - val_loss: 18.4087 - val_accuracy: 0.1654\n",
      "Epoch 16/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1230 - accuracy: 0.9833 - val_loss: 17.3617 - val_accuracy: 0.2835\n",
      "Epoch 17/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1091 - accuracy: 0.9699 - val_loss: 15.2176 - val_accuracy: 0.1890\n",
      "Epoch 18/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1077 - accuracy: 0.9766 - val_loss: 13.7709 - val_accuracy: 0.1496\n",
      "Epoch 19/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1026 - accuracy: 0.9666 - val_loss: 9.2380 - val_accuracy: 0.2598\n",
      "Epoch 20/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1111 - accuracy: 0.9632 - val_loss: 12.0185 - val_accuracy: 0.1654\n",
      "Epoch 21/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1154 - accuracy: 0.9766 - val_loss: 6.9735 - val_accuracy: 0.2520\n",
      "Epoch 22/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.1662 - accuracy: 0.9632 - val_loss: 7.4994 - val_accuracy: 0.3858\n",
      "Epoch 23/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0928 - accuracy: 0.9766 - val_loss: 10.7742 - val_accuracy: 0.2677\n",
      "Epoch 24/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0794 - accuracy: 0.9866 - val_loss: 12.2819 - val_accuracy: 0.2598\n",
      "Epoch 25/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0645 - accuracy: 0.9799 - val_loss: 7.1365 - val_accuracy: 0.3386\n",
      "Epoch 26/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.1047 - accuracy: 0.9732 - val_loss: 5.3133 - val_accuracy: 0.3858\n",
      "Epoch 27/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0815 - accuracy: 0.9632 - val_loss: 5.9641 - val_accuracy: 0.2992\n",
      "Epoch 28/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0477 - accuracy: 0.9933 - val_loss: 5.9377 - val_accuracy: 0.3465\n",
      "Epoch 29/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0379 - accuracy: 0.9967 - val_loss: 3.1185 - val_accuracy: 0.4331\n",
      "Epoch 30/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0268 - accuracy: 0.9967 - val_loss: 4.6159 - val_accuracy: 0.2677\n",
      "Epoch 31/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0248 - accuracy: 0.9967 - val_loss: 2.2458 - val_accuracy: 0.5433\n",
      "Epoch 32/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.4730 - val_accuracy: 0.6220\n",
      "Epoch 33/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.2252 - val_accuracy: 0.6535\n",
      "Epoch 34/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0152 - accuracy: 0.9933 - val_loss: 1.3923 - val_accuracy: 0.6063\n",
      "Epoch 35/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0914 - accuracy: 0.9799 - val_loss: 6.5527 - val_accuracy: 0.2362\n",
      "Epoch 36/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0889 - accuracy: 0.9732 - val_loss: 3.8609 - val_accuracy: 0.2441\n",
      "Epoch 37/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0811 - accuracy: 0.9766 - val_loss: 6.9966 - val_accuracy: 0.2047\n",
      "Epoch 38/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1248 - accuracy: 0.9599 - val_loss: 5.4168 - val_accuracy: 0.3465\n",
      "Epoch 39/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0821 - accuracy: 0.9833 - val_loss: 9.1019 - val_accuracy: 0.1417\n",
      "Epoch 40/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1197 - accuracy: 0.9699 - val_loss: 3.3426 - val_accuracy: 0.2756\n",
      "Epoch 41/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0640 - accuracy: 0.9933 - val_loss: 2.1167 - val_accuracy: 0.4961\n",
      "Epoch 42/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0509 - accuracy: 0.9933 - val_loss: 3.6919 - val_accuracy: 0.2441\n",
      "Epoch 43/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0336 - accuracy: 0.9933 - val_loss: 2.8268 - val_accuracy: 0.4252\n",
      "Epoch 44/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 2.3201 - val_accuracy: 0.4961\n",
      "Epoch 45/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 1.7541 - val_accuracy: 0.5512\n",
      "Epoch 46/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 1.5610 - val_accuracy: 0.6063\n",
      "Epoch 47/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0103 - accuracy: 0.9967 - val_loss: 1.5451 - val_accuracy: 0.5906\n",
      "Epoch 48/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0337 - accuracy: 0.9933 - val_loss: 1.5485 - val_accuracy: 0.5512\n",
      "Epoch 49/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0362 - accuracy: 0.9933 - val_loss: 1.2977 - val_accuracy: 0.6220\n",
      "Epoch 50/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0333 - accuracy: 0.9933 - val_loss: 2.2309 - val_accuracy: 0.5669\n",
      "Epoch 51/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0758 - accuracy: 0.9799 - val_loss: 2.1605 - val_accuracy: 0.5669\n",
      "Epoch 52/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0701 - accuracy: 0.9799 - val_loss: 1.5562 - val_accuracy: 0.5669\n",
      "Epoch 53/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0475 - accuracy: 0.9833 - val_loss: 1.5051 - val_accuracy: 0.6535\n",
      "Epoch 54/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0296 - accuracy: 0.9933 - val_loss: 2.0210 - val_accuracy: 0.5748\n",
      "Epoch 55/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.6620 - val_accuracy: 0.6614\n",
      "Epoch 56/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.7549 - val_accuracy: 0.6614\n",
      "Epoch 57/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.5007 - val_accuracy: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/270\n",
      "10/10 [==============================] - 2s 203ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.6403 - val_accuracy: 0.6614\n",
      "Epoch 59/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.5398 - val_accuracy: 0.6378\n",
      "Epoch 60/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.5082 - val_accuracy: 0.6220\n",
      "Epoch 61/270\n",
      "10/10 [==============================] - 2s 204ms/step - loss: 0.0168 - accuracy: 0.9967 - val_loss: 4.8430 - val_accuracy: 0.2756\n",
      "Epoch 62/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0172 - accuracy: 0.9967 - val_loss: 1.7669 - val_accuracy: 0.6378\n",
      "Epoch 63/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 3.0278 - val_accuracy: 0.5039\n",
      "Epoch 64/270\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 4.3749 - val_accuracy: 0.3386\n",
      "Epoch 65/270\n",
      "10/10 [==============================] - 2s 201ms/step - loss: 0.0360 - accuracy: 0.9866 - val_loss: 2.4544 - val_accuracy: 0.5827\n",
      "Epoch 66/270\n",
      "10/10 [==============================] - 2s 203ms/step - loss: 0.0750 - accuracy: 0.9799 - val_loss: 2.1338 - val_accuracy: 0.5512\n",
      "Epoch 67/270\n",
      "10/10 [==============================] - 3s 232ms/step - loss: 0.0456 - accuracy: 0.9900 - val_loss: 1.5138 - val_accuracy: 0.6693\n",
      "Epoch 68/270\n",
      "10/10 [==============================] - 2s 201ms/step - loss: 0.0714 - accuracy: 0.9799 - val_loss: 2.3963 - val_accuracy: 0.4882\n",
      "Epoch 69/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0751 - accuracy: 0.9799 - val_loss: 2.0814 - val_accuracy: 0.4803\n",
      "Epoch 70/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0590 - accuracy: 0.9799 - val_loss: 2.2287 - val_accuracy: 0.5512\n",
      "Epoch 71/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0891 - accuracy: 0.9732 - val_loss: 3.7906 - val_accuracy: 0.4961\n",
      "Epoch 72/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0515 - accuracy: 0.9933 - val_loss: 1.7588 - val_accuracy: 0.5984\n",
      "Epoch 73/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0489 - accuracy: 0.9933 - val_loss: 2.2046 - val_accuracy: 0.5748\n",
      "Epoch 74/270\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 0.0407 - accuracy: 0.9900 - val_loss: 2.8807 - val_accuracy: 0.4409\n",
      "Epoch 75/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0561 - accuracy: 0.9766 - val_loss: 3.2359 - val_accuracy: 0.4016\n",
      "Epoch 76/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0384 - accuracy: 0.9900 - val_loss: 3.4711 - val_accuracy: 0.3307\n",
      "Epoch 77/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0409 - accuracy: 0.9967 - val_loss: 2.6327 - val_accuracy: 0.4724\n",
      "Epoch 78/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0189 - accuracy: 0.9967 - val_loss: 1.7448 - val_accuracy: 0.6142\n",
      "Epoch 79/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 2.4521 - val_accuracy: 0.5748\n",
      "Epoch 80/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 2.5605 - val_accuracy: 0.5354\n",
      "Epoch 81/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.8310 - val_accuracy: 0.5984\n",
      "Epoch 82/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.6822 - val_accuracy: 0.6142\n",
      "Epoch 83/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.7071 - val_accuracy: 0.6693\n",
      "Epoch 84/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.7943 - val_accuracy: 0.6063\n",
      "Epoch 85/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.7723 - val_accuracy: 0.6299\n",
      "Epoch 86/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.6801 - val_accuracy: 0.6142\n",
      "Epoch 87/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.6071 - val_accuracy: 0.6299\n",
      "Epoch 88/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.5310 - val_accuracy: 0.6772\n",
      "Epoch 89/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.4408 - val_accuracy: 0.6850\n",
      "Epoch 90/270\n",
      "10/10 [==============================] - 2s 200ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.3460 - val_accuracy: 0.7244\n",
      "Epoch 91/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.3220 - val_accuracy: 0.6850\n",
      "Epoch 92/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 8.6218e-04 - accuracy: 1.0000 - val_loss: 1.3181 - val_accuracy: 0.6535\n",
      "Epoch 93/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.2990 - val_accuracy: 0.6535\n",
      "Epoch 94/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.2861 - val_accuracy: 0.6457\n",
      "Epoch 95/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.2940 - val_accuracy: 0.6614\n",
      "Epoch 96/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.4181 - val_accuracy: 0.6693\n",
      "Epoch 97/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.3244 - val_accuracy: 0.6693\n",
      "Epoch 98/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3233 - val_accuracy: 0.6850\n",
      "Epoch 99/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.5862 - val_accuracy: 0.6772\n",
      "Epoch 100/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.4358 - val_accuracy: 0.6535\n",
      "Epoch 101/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.4407 - val_accuracy: 0.6693\n",
      "Epoch 102/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.4563 - val_accuracy: 0.6535\n",
      "Epoch 103/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.4433 - val_accuracy: 0.6535\n",
      "Epoch 104/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.4186 - val_accuracy: 0.6614\n",
      "Epoch 105/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 7.6505e-04 - accuracy: 1.0000 - val_loss: 1.3977 - val_accuracy: 0.6535\n",
      "Epoch 106/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 8.4981e-04 - accuracy: 1.0000 - val_loss: 1.3927 - val_accuracy: 0.6614\n",
      "Epoch 107/270\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 5.6136e-04 - accuracy: 1.0000 - val_loss: 1.3866 - val_accuracy: 0.6614\n",
      "Epoch 108/270\n",
      "10/10 [==============================] - 2s 200ms/step - loss: 8.1655e-04 - accuracy: 1.0000 - val_loss: 1.3855 - val_accuracy: 0.6457\n",
      "Epoch 109/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 9.8025e-04 - accuracy: 1.0000 - val_loss: 1.3945 - val_accuracy: 0.6614\n",
      "Epoch 110/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 4.9095e-04 - accuracy: 1.0000 - val_loss: 1.3973 - val_accuracy: 0.6535\n",
      "Epoch 111/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.3798 - val_accuracy: 0.6614\n",
      "Epoch 112/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 6.0414e-04 - accuracy: 1.0000 - val_loss: 1.3647 - val_accuracy: 0.6535\n",
      "Epoch 113/270\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 6.9805e-04 - accuracy: 1.0000 - val_loss: 1.3548 - val_accuracy: 0.6772\n",
      "Epoch 114/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 8.7039e-04 - accuracy: 1.0000 - val_loss: 1.3458 - val_accuracy: 0.6772\n",
      "Epoch 115/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.3360 - val_accuracy: 0.6614\n",
      "Epoch 116/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 7.3042e-04 - accuracy: 1.0000 - val_loss: 1.3305 - val_accuracy: 0.6378\n",
      "Epoch 117/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 8.1987e-04 - accuracy: 1.0000 - val_loss: 1.3237 - val_accuracy: 0.6378\n",
      "Epoch 118/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.3154 - val_accuracy: 0.6772\n",
      "Epoch 119/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 3.9282e-04 - accuracy: 1.0000 - val_loss: 1.3135 - val_accuracy: 0.6772\n",
      "Epoch 120/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 8.2940e-04 - accuracy: 1.0000 - val_loss: 1.3063 - val_accuracy: 0.6693\n",
      "Epoch 121/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 6.7492e-04 - accuracy: 1.0000 - val_loss: 1.3101 - val_accuracy: 0.6378\n",
      "Epoch 122/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 8.2152e-04 - accuracy: 1.0000 - val_loss: 1.3144 - val_accuracy: 0.6850\n",
      "Epoch 123/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 3.0638e-04 - accuracy: 1.0000 - val_loss: 1.3141 - val_accuracy: 0.6850\n",
      "Epoch 124/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 5.3526e-04 - accuracy: 1.0000 - val_loss: 1.3172 - val_accuracy: 0.6693\n",
      "Epoch 125/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.3006 - val_accuracy: 0.6850\n",
      "Epoch 126/270\n",
      "10/10 [==============================] - 2s 201ms/step - loss: 0.0051 - accuracy: 0.9967 - val_loss: 2.6792 - val_accuracy: 0.4409\n",
      "Epoch 127/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.4132 - val_accuracy: 0.5591\n",
      "Epoch 128/270\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.1805 - val_accuracy: 0.5984\n",
      "Epoch 129/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.8405 - val_accuracy: 0.6299\n",
      "Epoch 130/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.5009 - val_accuracy: 0.6693\n",
      "Epoch 131/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.3547 - val_accuracy: 0.6850\n",
      "Epoch 132/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 9.9247e-04 - accuracy: 1.0000 - val_loss: 1.2989 - val_accuracy: 0.6850\n",
      "Epoch 133/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.2913 - val_accuracy: 0.6772\n",
      "Epoch 134/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0151 - accuracy: 0.9967 - val_loss: 1.7116 - val_accuracy: 0.6220\n",
      "Epoch 135/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0179 - accuracy: 0.9967 - val_loss: 1.6969 - val_accuracy: 0.6299\n",
      "Epoch 136/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0200 - accuracy: 0.9967 - val_loss: 2.0060 - val_accuracy: 0.6220\n",
      "Epoch 137/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.6614 - val_accuracy: 0.6457\n",
      "Epoch 138/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.9085 - val_accuracy: 0.5433\n",
      "Epoch 139/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.1865 - val_accuracy: 0.5591\n",
      "Epoch 140/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 2.6736 - val_accuracy: 0.6063\n",
      "Epoch 141/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.5516 - val_accuracy: 0.5906\n",
      "Epoch 142/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 2.6493 - val_accuracy: 0.5906\n",
      "Epoch 143/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0387 - accuracy: 0.9833 - val_loss: 3.0836 - val_accuracy: 0.5669\n",
      "Epoch 144/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0286 - accuracy: 0.9900 - val_loss: 3.7751 - val_accuracy: 0.5039\n",
      "Epoch 145/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.2766 - accuracy: 0.9565 - val_loss: 7.7588 - val_accuracy: 0.4882\n",
      "Epoch 146/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.1994 - accuracy: 0.9264 - val_loss: 7.6623 - val_accuracy: 0.4409\n",
      "Epoch 147/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.1461 - accuracy: 0.9498 - val_loss: 13.7136 - val_accuracy: 0.1654\n",
      "Epoch 148/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.1716 - accuracy: 0.9298 - val_loss: 12.3889 - val_accuracy: 0.1575\n",
      "Epoch 149/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0962 - accuracy: 0.9699 - val_loss: 15.5329 - val_accuracy: 0.1811\n",
      "Epoch 150/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0858 - accuracy: 0.9833 - val_loss: 9.6852 - val_accuracy: 0.2047\n",
      "Epoch 151/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0470 - accuracy: 0.9933 - val_loss: 8.3001 - val_accuracy: 0.2205\n",
      "Epoch 152/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 4.5760 - val_accuracy: 0.3937\n",
      "Epoch 153/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0110 - accuracy: 0.9967 - val_loss: 4.2708 - val_accuracy: 0.4173\n",
      "Epoch 154/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0245 - accuracy: 0.9933 - val_loss: 3.9457 - val_accuracy: 0.4016\n",
      "Epoch 155/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 4.8003 - val_accuracy: 0.3307\n",
      "Epoch 156/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0198 - accuracy: 0.9933 - val_loss: 3.4786 - val_accuracy: 0.4094\n",
      "Epoch 157/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.3603 - val_accuracy: 0.4331\n",
      "Epoch 158/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 3.0358 - val_accuracy: 0.4882\n",
      "Epoch 159/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.0074 - val_accuracy: 0.4961\n",
      "Epoch 160/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.9397 - val_accuracy: 0.5118\n",
      "Epoch 161/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.8765 - val_accuracy: 0.4803\n",
      "Epoch 162/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.8191 - val_accuracy: 0.4803\n",
      "Epoch 163/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.7697 - val_accuracy: 0.5039\n",
      "Epoch 164/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.6969 - val_accuracy: 0.5039\n",
      "Epoch 165/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.5895 - val_accuracy: 0.5276\n",
      "Epoch 166/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.5138 - val_accuracy: 0.5276\n",
      "Epoch 167/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.4852 - val_accuracy: 0.5354\n",
      "Epoch 168/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.4252 - val_accuracy: 0.5197\n",
      "Epoch 169/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.2876 - val_accuracy: 0.5276\n",
      "Epoch 170/270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 193ms/step - loss: 8.2444e-04 - accuracy: 1.0000 - val_loss: 2.2119 - val_accuracy: 0.5433\n",
      "Epoch 171/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.1470 - val_accuracy: 0.5433\n",
      "Epoch 172/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 9.5831e-04 - accuracy: 1.0000 - val_loss: 2.1153 - val_accuracy: 0.5669\n",
      "Epoch 173/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0059 - accuracy: 0.9967 - val_loss: 2.2913 - val_accuracy: 0.5512\n",
      "Epoch 174/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.7174 - val_accuracy: 0.4803\n",
      "Epoch 175/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 2.9286 - val_accuracy: 0.5039\n",
      "Epoch 176/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.7330 - val_accuracy: 0.4409\n",
      "Epoch 177/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 4.0214 - val_accuracy: 0.4331\n",
      "Epoch 178/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.7124 - val_accuracy: 0.4331\n",
      "Epoch 179/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 9.5053e-04 - accuracy: 1.0000 - val_loss: 3.2267 - val_accuracy: 0.4646\n",
      "Epoch 180/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.8175 - val_accuracy: 0.5276\n",
      "Epoch 181/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.3343 - val_accuracy: 0.5984\n",
      "Epoch 182/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 9.7942e-04 - accuracy: 1.0000 - val_loss: 2.2839 - val_accuracy: 0.5984\n",
      "Epoch 183/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.1553 - val_accuracy: 0.6063\n",
      "Epoch 184/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.0153 - val_accuracy: 0.5984\n",
      "Epoch 185/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 7.9881e-04 - accuracy: 1.0000 - val_loss: 1.8980 - val_accuracy: 0.5906\n",
      "Epoch 186/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 9.3710e-04 - accuracy: 1.0000 - val_loss: 1.8496 - val_accuracy: 0.5984\n",
      "Epoch 187/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 4.8827e-04 - accuracy: 1.0000 - val_loss: 1.8185 - val_accuracy: 0.5984\n",
      "Epoch 188/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 4.0358e-04 - accuracy: 1.0000 - val_loss: 1.7808 - val_accuracy: 0.5984\n",
      "Epoch 189/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 7.0651e-04 - accuracy: 1.0000 - val_loss: 1.7573 - val_accuracy: 0.6220\n",
      "Epoch 190/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 5.0949e-04 - accuracy: 1.0000 - val_loss: 1.7703 - val_accuracy: 0.6220\n",
      "Epoch 191/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 4.5321e-04 - accuracy: 1.0000 - val_loss: 1.7489 - val_accuracy: 0.6299\n",
      "Epoch 192/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 5.1856e-04 - accuracy: 1.0000 - val_loss: 1.7161 - val_accuracy: 0.6220\n",
      "Epoch 193/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 3.3294e-04 - accuracy: 1.0000 - val_loss: 1.7029 - val_accuracy: 0.6535\n",
      "Epoch 194/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 5.6694e-04 - accuracy: 1.0000 - val_loss: 1.6735 - val_accuracy: 0.6457\n",
      "Epoch 195/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.6187 - val_accuracy: 0.6378\n",
      "Epoch 196/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.5815 - val_accuracy: 0.6142\n",
      "Epoch 197/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 5.3574e-04 - accuracy: 1.0000 - val_loss: 1.9220 - val_accuracy: 0.5276\n",
      "Epoch 198/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0131 - accuracy: 0.9967 - val_loss: 1.6670 - val_accuracy: 0.5906\n",
      "Epoch 199/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1035 - accuracy: 0.9799 - val_loss: 3.6045 - val_accuracy: 0.3228\n",
      "Epoch 200/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0874 - accuracy: 0.9666 - val_loss: 25.2030 - val_accuracy: 0.2283\n",
      "Epoch 201/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0829 - accuracy: 0.9732 - val_loss: 16.1108 - val_accuracy: 0.2677\n",
      "Epoch 202/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0486 - accuracy: 0.9799 - val_loss: 8.3710 - val_accuracy: 0.3071\n",
      "Epoch 203/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.1304 - accuracy: 0.9632 - val_loss: 23.1530 - val_accuracy: 0.2126\n",
      "Epoch 204/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0876 - accuracy: 0.9666 - val_loss: 12.5505 - val_accuracy: 0.3150\n",
      "Epoch 205/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0409 - accuracy: 0.9900 - val_loss: 9.7270 - val_accuracy: 0.4173\n",
      "Epoch 206/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0368 - accuracy: 0.9866 - val_loss: 7.8932 - val_accuracy: 0.2520\n",
      "Epoch 207/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0345 - accuracy: 0.9900 - val_loss: 3.9948 - val_accuracy: 0.3465\n",
      "Epoch 208/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0760 - accuracy: 0.9666 - val_loss: 4.6092 - val_accuracy: 0.4882\n",
      "Epoch 209/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0805 - accuracy: 0.9799 - val_loss: 2.0368 - val_accuracy: 0.6063\n",
      "Epoch 210/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0476 - accuracy: 0.9799 - val_loss: 8.2680 - val_accuracy: 0.3307\n",
      "Epoch 211/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 9.2943 - val_accuracy: 0.3386\n",
      "Epoch 212/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 8.0915 - val_accuracy: 0.4094\n",
      "Epoch 213/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0210 - accuracy: 0.9933 - val_loss: 6.5815 - val_accuracy: 0.4488\n",
      "Epoch 214/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 6.1833 - val_accuracy: 0.4567\n",
      "Epoch 215/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 4.8654 - val_accuracy: 0.4961\n",
      "Epoch 216/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.4277 - val_accuracy: 0.5354\n",
      "Epoch 217/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6996 - val_accuracy: 0.6063\n",
      "Epoch 218/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.3596 - val_accuracy: 0.6220\n",
      "Epoch 219/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.3634 - val_accuracy: 0.6063\n",
      "Epoch 220/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0158 - accuracy: 0.9933 - val_loss: 3.1786 - val_accuracy: 0.5197\n",
      "Epoch 221/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0398 - accuracy: 0.9933 - val_loss: 6.8740 - val_accuracy: 0.4016\n",
      "Epoch 222/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0263 - accuracy: 0.9900 - val_loss: 6.8544 - val_accuracy: 0.2913\n",
      "Epoch 223/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0256 - accuracy: 0.9933 - val_loss: 5.0193 - val_accuracy: 0.3543\n",
      "Epoch 224/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 2.8342 - val_accuracy: 0.5433\n",
      "Epoch 225/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.8619 - val_accuracy: 0.6142\n",
      "Epoch 226/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.9383 - val_accuracy: 0.6378\n",
      "Epoch 227/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.0329 - val_accuracy: 0.5669\n",
      "Epoch 228/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.0123 - val_accuracy: 0.6220\n",
      "Epoch 229/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.3457 - val_accuracy: 0.4961\n",
      "Epoch 230/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.8037 - val_accuracy: 0.5039\n",
      "Epoch 231/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.1853 - val_accuracy: 0.5591\n",
      "Epoch 232/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0103 - accuracy: 0.9933 - val_loss: 1.7438 - val_accuracy: 0.6063\n",
      "Epoch 233/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0488 - accuracy: 0.9833 - val_loss: 6.5969 - val_accuracy: 0.2835\n",
      "Epoch 234/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0874 - accuracy: 0.9799 - val_loss: 4.6609 - val_accuracy: 0.3701\n",
      "Epoch 235/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0551 - accuracy: 0.9866 - val_loss: 6.3067 - val_accuracy: 0.3543\n",
      "Epoch 236/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0529 - accuracy: 0.9732 - val_loss: 4.1780 - val_accuracy: 0.3228\n",
      "Epoch 237/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0687 - accuracy: 0.9766 - val_loss: 3.9325 - val_accuracy: 0.3307\n",
      "Epoch 238/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0527 - accuracy: 0.9766 - val_loss: 3.2994 - val_accuracy: 0.3543\n",
      "Epoch 239/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0217 - accuracy: 0.9967 - val_loss: 5.1969 - val_accuracy: 0.2283\n",
      "Epoch 240/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0175 - accuracy: 0.9933 - val_loss: 3.5360 - val_accuracy: 0.3543\n",
      "Epoch 241/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0176 - accuracy: 0.9933 - val_loss: 2.1486 - val_accuracy: 0.4488\n",
      "Epoch 242/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0128 - accuracy: 0.9967 - val_loss: 2.0227 - val_accuracy: 0.5591\n",
      "Epoch 243/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.9758 - val_accuracy: 0.5669\n",
      "Epoch 244/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.4029 - val_accuracy: 0.5197\n",
      "Epoch 245/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.3534 - val_accuracy: 0.5433\n",
      "Epoch 246/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0198 - accuracy: 0.9933 - val_loss: 2.4559 - val_accuracy: 0.4882\n",
      "Epoch 247/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0659 - accuracy: 0.9666 - val_loss: 6.9187 - val_accuracy: 0.1732\n",
      "Epoch 248/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0561 - accuracy: 0.9799 - val_loss: 4.3486 - val_accuracy: 0.4409\n",
      "Epoch 249/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0901 - accuracy: 0.9732 - val_loss: 12.8086 - val_accuracy: 0.2126\n",
      "Epoch 250/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0343 - accuracy: 0.9933 - val_loss: 6.6837 - val_accuracy: 0.3307\n",
      "Epoch 251/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0249 - accuracy: 0.9900 - val_loss: 6.5489 - val_accuracy: 0.4252\n",
      "Epoch 252/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0275 - accuracy: 0.9933 - val_loss: 2.3156 - val_accuracy: 0.5906\n",
      "Epoch 253/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0217 - accuracy: 0.9866 - val_loss: 1.9727 - val_accuracy: 0.6142\n",
      "Epoch 254/270\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 2.2951 - val_accuracy: 0.6378\n",
      "Epoch 255/270\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0052 - accuracy: 0.9967 - val_loss: 1.7107 - val_accuracy: 0.6535\n",
      "Epoch 256/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.5911 - val_accuracy: 0.6535\n",
      "Epoch 257/270\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.5453 - val_accuracy: 0.6457\n",
      "Epoch 258/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.6105 - val_accuracy: 0.6378\n",
      "Epoch 259/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.6198 - val_accuracy: 0.6142\n",
      "Epoch 260/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.6325 - val_accuracy: 0.6142\n",
      "Epoch 261/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.6101 - val_accuracy: 0.5984\n",
      "Epoch 262/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 8.5421e-04 - accuracy: 1.0000 - val_loss: 1.5623 - val_accuracy: 0.6142\n",
      "Epoch 263/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.4965 - val_accuracy: 0.6457\n",
      "Epoch 264/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.4902 - val_accuracy: 0.6220\n",
      "Epoch 265/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 9.8461e-04 - accuracy: 1.0000 - val_loss: 1.8566 - val_accuracy: 0.6299\n",
      "Epoch 266/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.9007 - val_accuracy: 0.6220\n",
      "Epoch 267/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.6180 - val_accuracy: 0.6378\n",
      "Epoch 268/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.4869 - val_accuracy: 0.6378\n",
      "Epoch 269/270\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 8.1967e-04 - accuracy: 1.0000 - val_loss: 1.4229 - val_accuracy: 0.6457\n",
      "Epoch 270/270\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.4469 - val_accuracy: 0.6378\n"
     ]
    }
   ],
   "source": [
    "epochs =270\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=epochs,\n",
    "    validation_data=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0931 - categorical_accuracy: 0.6378\n",
      "Test Accuracy: 63.78%\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 0s - loss: 0.0931 - categorical_accuracy: 0.6378 - 242ms/epoch - 61ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 0s - loss: 1.4469 - accuracy: 0.6378 - 499ms/epoch - 125ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 222ms/step - loss: 5.5227e-04 - accuracy: 1.0000\n",
      "Train Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(train_dataset)\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Not Ripe', 'A Ripe', 'B Not Ripe', 'B Ripe', 'C Not Ripe', 'C Ripe']\n"
     ]
    }
   ],
   "source": [
    "class_names = test_dataset.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.09 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 A Ripe\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR10_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 34.23 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR11_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 29.47 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR12_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.16 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR13_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR14_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 23.06 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR15_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.08 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR16_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.20 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR1_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR2_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 32.29 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR3_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 22.84 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR4_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 34.96 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR5_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.20 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR6_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 27.91 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR77_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.20 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR78_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR7_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR7_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 32.80 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR8_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 34.91 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Ripe/AR9_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 A Not Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR10_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR11_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR12_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 30.78 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR13_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.18 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR14_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160,160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR15_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.19 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR16_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR1_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR2_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR3_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR4_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 34.92 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR5_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR6_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR79_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR7_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR80_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR81_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.11 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR8_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/A Not Ripe/ANR9_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 35.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 B Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR10_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 29.03 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR11_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 31.37 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR12_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 29.81 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR13_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 34.55 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR14_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 34.79 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR15_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 26.67 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR16_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 28.67 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR1_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 20.69 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR2_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 34.51 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR3_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 28.42 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR4_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 34.12 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR5_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR65_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 34.82 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR66_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 34.28 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR67_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 33.94 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR6_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 22.61 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR7_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 22.85 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR8_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to A Ripe with a 22.60 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Ripe/BR9_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 33.82 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 B Not Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR10_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 30.85 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR11_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 31.68 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR12_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 19.69 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR13_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR14_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 20.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR15_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 19.62 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR16_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR1_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR2_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 34.08 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR3_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.01 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR4_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 30.40 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR5_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 20.63 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR69_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 34.74 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR6_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 33.99 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR70_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Ripe with a 27.66 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR71_270.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to B Not Ripe with a 35.19 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR7_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 21.66 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR8_180.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 30.62 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/B Not Ripe/BNR9_90.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 C Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR11_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 30.31 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR13_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR22_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR24_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR30_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR32_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR38_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 23.42 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR44_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR45_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR46_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR47_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR4_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR50_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR51_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.19 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR60_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.19 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR62_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.20 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR63_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR64_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Ripe/CR8_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#1 C Not Ripe\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR10_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR11_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR1_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR20_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.13 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR22_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR28_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR30_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR38_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 27.67 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR43_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR52_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR57_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.16 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR58_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR59_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR5_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR62_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.22 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR63_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Not Ripe with a 35.21 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR64_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 34.96 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR6_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to C Ripe with a 31.81 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "\n",
    "rootdir = \"C:/Users/User/Desktop/Category/test/C Not Ripe/CNR9_0.png\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    rootdir, target_size=(160, 160)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, axis=0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAHiCAYAAADbK6SdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAB/4klEQVR4nO2deZwcZZ3/39/uniuZ3PdJEgmEBEiAiAoIQVZBkR+sisJ6oOCCiuLiugqeuC67rq7rsS6yeMG6usDiIlkFDxAMCALhCoQzkASG3PdMkjm6+/n9UVXd1dVV1dU93dNVM9/36zWv6q6urn66e/r51Pd8xBiDoiiKoijNI9XsASiKoijKSEfFWFEURVGajIqxoiiKojQZFWNFURRFaTIqxoqiKIrSZFSMFUVRFKXJqBgriqIoSpNRMU4QInKPiOwWkbZmj0VRlKFFRDaIyF80exxKY1AxTggiMg94I2CA/zeEr5sZqtdSFEUZqagYJ4cPAH8GrgcucHaKyBwR+V8R2S4iO0Xke67H/lpEnhGRbhF5WkSOtfcbETnUddz1IvIP9u0VItIlIp8VkS3AT0Rkgoj8yn6N3fbt2a7nTxSRn4jIJvvxX9r7nxKRs1zHtYjIDhFZ1qDPSFFGFCLSJiLftn97m+zbbfZjk+3f6h4R2SUi94pIyn7ssyLyqj03PCcipzX3nSgqxsnhA8DP7L/TRWSaiKSBXwEbgXnALOBGABE5F7jKft5YLGt6Z8TXmg5MBA4BLsb6P/mJfX8ucBD4nuv4nwKjgCXAVOBb9v7/BN7nOu5twGZjzOMRx6EoSjifB14PLAOWAscDX7Af+1ugC5gCTAM+BxgRORz4OPBaY8wY4HRgw5COWilDXZAJQEROwhLCm40xO0TkReCvsCzlmcDfGWOy9uH32dsPA183xjxs319XxUvmgS8bY/rs+weBX7jGczVwt317BvBWYJIxZrd9yB/t7X8BXxSRscaYfcD7sYRbUZT68F7gE8aYbQAi8hXgP4AvAgPADOAQY8w64F77mBzQBiwWke3GmA3NGLhSilrGyeAC4HfGmB32/Z/b++YAG11C7GYO8GKNr7fdGNPr3BGRUSLyHyKyUUT2AauA8bZlPgfY5RLiAsaYTcCfgHeKyHgs0f5ZjWNSFKWcmVieMYeN9j6Ab2BdhP9ORF4SkSsAbGH+GyzP2TYRuVFEZqI0FRXjmCMiHcC7gVNEZIsdx70cyyW1FZgbkGT1CvCagNMewHIrO0z3PO5dyutvgcOB1xljxgInO8OzX2eiLbZ+3IDlqj4XeMAY82rAcYqiVM8mLK+Zw1x7H8aYbmPM3xpjFgBnAZ9yYsPGmJ8bYxyPmwH+eWiHrXhRMY4/5wA5YDFWXGgZcASWy+kcYDPwNREZLSLtInKi/bwfAp8WkePE4lARcX60jwN/JSJpETkDOKXCGMZguar3iMhE4MvOA8aYzcAdwDV2oleLiJzseu4vgWOBT2LFkBVFqZ0W+3feLiLtwH8DXxCRKSIyGfgSVngIEXm7/bsXYB/WPJITkcNF5E12olcv1m8715y3ozioGMefC4CfGGNeNsZscf6wEqjOx7riPRR4GStZ4z0Axpj/Aa7Gcml3Y4niRPucn7Sftwcr5vTLCmP4NtAB7MCKU//G8/j7seJTzwLbsFxg2ONw4s3zgf+N/rYVRfHhdizxdP7agdXAGuBJ4FHgH+xjFwJ3Aj3AA8A1xph7sOLFX8P6PW/BSrr83JC9A8UXMcbrkVSU+iIiXwIOM8a8r+LBiqIoIxDNplYaiu3WvgjLelYURVF8UDe10jBE5K+xErzuMMasavZ4FEVR4oq6qRVFURSlyahlrCiKoihNRsVYURRFUZpM0xK4Jk+ebObNm9esl1eUxPDII4/sMMZMafY4wtDfs6JEI+j33DQxnjdvHqtXr27WyytKYhCRjZWPai76e1aUaAT9ntVNrSiKoihNRsVYURRFUZqMirGiKIqiNBntwKUoiqJEYmBggK6uLnp7eysfPMJpb29n9uzZtLS0RDpexVhRFEWJRFdXF2PGjGHevHlYi0Epfhhj2LlzJ11dXcyfPz/Sc9RNrSiKokSit7eXSZMmqRBXQESYNGlSVR4EFWNFURQlMirE0aj2c1IxVhRFURLBzp07WbZsGcuWLWP69OnMmjWrcL+/vz/0uatXr+ayyy6r+BonnHBCvYZbFRozVhRFURLBpEmTePzxxwG46qqr6Ozs5NOf/nTh8Ww2SybjL2vLly9n+fLlFV/j/vvvr8tYq0UtY0VRFCWxfPCDH+RTn/oUp556Kp/97Gd56KGHOOGEEzjmmGM44YQTeO655wC45557ePvb3w5YQn7hhReyYsUKFixYwHe/+93C+To7OwvHr1ixgne9610sWrSI9773vTirHN5+++0sWrSIk046icsuu6xw3sGglrGiKIpSNV/5v7U8vWlfXc+5eOZYvnzWkqqf9/zzz3PnnXeSTqfZt28fq1atIpPJcOedd/K5z32OX/ziF2XPefbZZ7n77rvp7u7m8MMP56Mf/WhZGdJjjz3G2rVrmTlzJieeeCJ/+tOfWL58OZdccgmrVq1i/vz5nH/++TW/XzcqxoqiKEqiOffcc0mn0wDs3buXCy64gBdeeAERYWBgwPc5Z555Jm1tbbS1tTF16lS2bt3K7NmzS445/vjjC/uWLVvGhg0b6OzsZMGCBYWSpfPPP5/rrrtu0O9BxVhRFEWpmlos2EYxevTowu0vfvGLnHrqqdx6661s2LCBFStW+D6nra2tcDudTpPNZiMd47iq643GjBVFUZRhw969e5k1axYA119/fd3Pv2jRIl566SU2bNgAwE033VSX81YUYxH5sYhsE5GnAh4XEfmuiKwTkTUicmxdRqYoiqIoVfKZz3yGK6+8khNPPJFcLlf383d0dHDNNddwxhlncNJJJzFt2jTGjRs36PNKJZNbRE4GeoD/NMYc6fP424BPAG8DXgd8xxjzukovvHz5cqPrnypKZUTkEWNM5ZqMJqK/55HBM888wxFHHNHsYTSdnp4eOjs7McZw6aWXsnDhQi6//PKy4/w+r6Dfc8WYsTFmlYjMCznkbCyhNsCfRWS8iMwwxmyudO5m0JfNYQy0t6R9H8/lDT19WdpbUrRlyo850J9lIFe8gBnbniFvoHcgx+i2yiF45+Knpy9LvjGhh1DaMinSKeFAf+kV49j2DCJCXzZH70C+sH90a5pMutyB4rwPp8vM/r4s2QhvKJ0SRrem2ddbHp+JigiMactU7HBzoD9LazrFQM7Qnyu+J+e9Bo076D077O/LkjeGMe3lDeCNMRgD+/ujfb8taWFU6whI3ejdB5KCts5mj0RRBs0PfvADbrjhBvr7+znmmGO45JJLBn3OeswCs4BXXPe77H2xE+Pt3X2s+MbdAKz6zKlM6mwrO+avfvBnHly/iwmjWrj70ysYP6q18NgjG3dz7rX3l0yy733dXJ7atI8nXtnDDRcezymHTQl8fWMM5/z7n3iia2/93lSd+H9LZ/LFty/mTd+8h26XUB4/fyI3X/KGsuMv+ekjtGRS7Ds4wL0v7EAEGpTX4MslpyzgyrcGX6E/snEX7/z+A4B1AZBzfWkXvOEQvnL2kdzx5GY++rNHy567ZOZYfn3ZG33P+39PbOIT//0YAP9wzpG87/WHlDz+zu/fz6Mv74n8Pv7ymFl86z3LIh+fWH54GkxdDO++odkjUZRBc/nll/tawoOhHmLsZ574TssicjFwMcDcuXPr8NLRyOUN1/7xRf780k722xbhwxt2c8aR00uO6+4d4KENuzjx0En8ad1Ofvbgy1x66qGFx1c9vx2AL5x5BCLCfS9s52cPvlx4/KlX9waKcV82x6dufqIgxDPHtXPRGxfU9X1WwhjDP/z6GQAuOXkBU8e2A7B6wy7+b80mdvT00dOX5TNnHE5bJs3v1m7h6c3ldYT92Tx/fH47fdmitZkW4cozK7uv/u0PL7DnwABvPXI6y+dNrOl93PXMVn76wEY+dsqhjBvlvzzZC1t7CrdzecPn3raIdCrFysdf5e7ntvMV4O7ntjG2PcMn/+KwwrGPvrybX6/ZzLZ9vYXPxxjDfz6wkbcfPYPv/WEdCyaP5uBAjnue214mxo4QzxjXzocjfL+vmTK64jHDAyFgWlAUhfqIcRcwx3V/NrDJ70BjzHXAdWDFmOrw2pG4/cnNfOO3z9HRkuYdx87iV2s288jGXSVinM8bHt6wC2Pgo6ccSkqE6+/fwPvfcEjBJfroy7tZNH1sYZJ9y+JpvOva+5kwqpUNO/ezee/BstfuHcjR3pLm3ud38Os1lrPgkEmj+Ma7lnL8/NrEaDD05/I8+NIurnjrooKr9qylM1jTtZcnXtnDe5bP4WMrrAuQg/1ZHly/q/AeHJ7evK9EiAEyaeGikyovFXb4tDF85f/W8tVzjmSyj2ciCkfOHMt7rvszqzfu4rQjpvkes/tAaW3hxSe/BrC+56tvf4Yte3t5ZONuXjtvYsm4j3l5PL9es5lHX97NGUfOAGD9jv18eeVavrxyLQD/cu5SHnxpJ3c9uw1jDCKCMaZwoQfw7fcs43ULJtX0/oYlQ+06UZSEUQ8xXgl8XERuxErg2hu3ePHPHtzI/MmjuetTp5BKCRt3HuAxjyvx3P94gEc27iYlsHTOOC4+eQHv/9FDHH3V7zhr6Uy+855lPPbyHs45ZmbhOXMmjuLBz/0FAGd8exWb95Qul/XN3z3Hv/1hHX/75sM4MJAjkxKe+srpgfHqoeBjKw7lYytK900d086frnhT2bGOWO7o6WP2hFGF/Y9u3A1YFyO/e3orAMfOnRDp9U9aOJnff+qUGkZeZOb4jsK4gthzoNg0fvaEjsLt4+ZZ43z9P90FwDuOLS3yXzJzLK2ZFI++vKcgxrtd55o2to3/t3QmA7k8//NIFy/vOsBvntrCP93xbOGY75ynQlyGpFDLWFGCqSjGIvLfwApgsoh0AV8GWgCMMdcCt2NlUq8DDgAfatRga2Xz3l6Wzh5PKmVZggsmj2bVC9tLjnlm8z6On29ZSWPaWzjp0Ml8/V1H89MHNvL4K7vZsq+Xnr4si6aP9X2NmeM72LS3l/99tItTDpvCpM421tqt4n5w70vMnTSKJbPGNVWIq8UR4+/c+QL/9I6jCklNL2zrYcKoFr5x7lLOWbeD1nSKYw+JJsb1YMoY5yKhKJLbunv543PWd7ri8Kns2t/P9LHt/Mu5S3nN1KIr+Jg54/n4qYfyvbvXAfD2o2eUnLstk2bB5NGs21Z0c+/eP1A49sKT5tOaSTFjXHthDM65HGaM60DxopaxooQRJZs6tPGmnUV9ad1G1AD295VmOs8Y38G27j4Gcnla0ikGcnkO9Oc46dDJnL7Ecl2LCO9ePoeXtu/nR/e9xIvbrcl5QUCMb+b4dv7w7DY+dfMTvHHhZH560evYtd8Si329WZ56dR/ve/3QxcnrgSN6//NIF6cumsrbjnLctj0smNLJuI6Wwr6hpL0lTWdbhu3dRcv4u3e9wH/92Yrfv//1h7D7wADjR7Vw0sLJJc8VET59+uG8svsAnW0ZDplU/n2+ZkonazcVk+x22ZbxZ89YxJyJlofAyaTu6cuybM547n1hR+H4mePb6/ROhxHqplbqwM6dOznttNMA2LJlC+l0milTrDydhx56iNbW1rCnc88999Da2tq0ZRLDGLY1FT99YAO/ePRVbv3YCezvy9LZVrRIZ45rxxjYsreXORNHse+gZfmM6yhPBpo5vp2BnOHh9bsAWDDZvzTDbQ29tH0/YLlKl80Zz+Ov7AFg1vhRfk+NLZPHFGO6mVQxT++l7ftDs8aHgilj2krc1O2uMrT2lhS7D/QzcXTwD/M75x0T+NiCKaP5zdotnPKNu3nL4mmFi5LxrmSxMe3WT6e7d4A9BwZYcfgU1m7ax/buPqaNVTEuQzSBSxk8lZZQrMQ999xDZ2dnLMV4WLTDvOPJzez0xA/vfWEHj7+yhw07D3BwoNwyBst9PZDL86P71gP+YuyI7H3rdjCqNc20sf5JR4tnlLuvdx8Y4IgZYwr3k2YxTXKJWa+dsNXdO8C27j7mNzkLeHJna4kYHxwoJk/1DuTZfaCfCaPCr5KDWDBlNLm8YePOA/zg3vXsPjBAJiV0uv6HHDHu6c0WXmvlx0/kxx9cTktIjfLIRcDkKx+mKFXyyCOPcMopp3Dcccdx+umns3mzlbL03e9+l8WLF3P00Udz3nnnsWHDBq699lq+9a1vsWzZMu69994mj7yUxFvGu/b389GfPcrx8yZy80eK9bDrd1jW6b12bNg9kc60432b9x5k7Z/3cs09LwIwtqP843Big4++vIfXzpsQ2GjimLnjC7e7ewfI5vLsPTjA1DFFAU5aLLG9JV2o0T3QZ9Uev7zrAADzfNy7Q8nkzjZecMV19/VmmT95NMYY9h60rNUJo/3Lnipx5MzS1nZ7DvQzYXRryXfv/D9192bZY7vEZ4zrSNx3PGSom3r4cccVsOXJ+p5z+lHw1q9FPtwYwyc+8Qluu+02pkyZwk033cTnP/95fvzjH/O1r32N9evX09bWxp49exg/fjwf+chHqramh4rEi/GmPVY50Qvbugv7HKsGirXB7i5HjmXctftgSTMIP8t41vji5PqBN8wLHIe7Oci+3iyHfv4OACa4XJuOsCeJR7/wZpb+/e8KZTtOxvjM8c0Vncmdbdz/4k5ue/xVfnTfesZ1tDC2PYMBVj5hVdZNrNEyXjhtTMn9Xfv7S75HgNGtGUTg6tufGdRrjRg0m1ppAH19fTz11FO8+c1vBiCXyzFjhpXHcvTRR/Pe976Xc845h3POOaeJo4xG4sV4815LHHYfGOCRjbuZ3NnKQ+t30Z/LIwKrnrcSa0a7YsadbRnmTOzgtsdfLbHwxvq0N5wwupV/fudRdPdmKyYr/eKjJ7Dv4ABruvbyrTufLzzfYXoCxXiU/bk5lrFTSz2zye9l+rh29h4c4JZHuljTtZdDJo1i7sTSmPx7jq89Ye7uT6/g7/7nCVZv3M1v124tqwlPpYSUCDnb2hsfEp9WQN3Uw5AqLNhGYYxhyZIlPPDAA2WP/frXv2bVqlWsXLmSr371q6xdu7YJI4zOMBDjYqONd37//pLH3rBgEve/uBModVODVRd72+ObeN7VqcnPMgZ4z2ujTerH2eU9py6ays2rX+HVPQeZMKqVv37jfG5e3ZXIWGJLOkVrJkVPvyXGm/b20pKWmht21AvHy+B8vxt3HuCoWeMKntB3L59d4tWolvmTR/PBE+ex2q6p9rqugRKvytj2xP+UGou6qZUG0NbWxvbt23nggQd4wxvewMDAAM8//zxHHHEEr7zyCqeeeionnXQSP//5z+np6WHMmDHs21feVTAOJE8dPGzyNNpwc9bSYoMO7yIOr5lSnhU9NkCMa+F1CyxLqjWT4vNnLuaJL7+lbuceaka3pjnQ57ipDzJtbHuhZrtZOLHZEkHsaKGjNV3y+GBwsqJPOnQyXzpr8aDPN7LRbGql/qRSKW655RY++9nPsnTpUpYtW8b9999PLpfjfe97H0cddRTHHHMMl19+OePHj+ess87i1ltv1QSuRuDXgtJhxeHF8huvZfyhE+dx8+pX6NpdfH49G3J89ewjWTZnPMfX2H85ToxqzbDfZRnPjEGSkl9m+riOFl6xE8zqkbl+7NwJXHXWYt553OzQ4y46aT5nNqHeOlFISi1jpa5cddVVhdurVq0qe/y+++4r23fYYYexZs2aRg6rZhJtGWdzeVZv2B34+HRXvafXMh7T3sKn33J44X5rnV3Io9syfOAN85puQdaD0W1Fy3hHdx9TAsq7hhIn/u7+3sZ1tLDfjm1PHD34MaZTwgdPnO+7VKKbz5xxeOiSiwq2m1pjxooSRKJnkNuf2sKrew7ygw8sZ+1XTi973F2K4k7gcnDHPZ+/+q2NGeQwwG0ZHxzIMSoGLT3bMmkmd7aVJFaNbW8phBq82c+NHotSCXVTK0oYiXVTG2O4btWLLJg8mtMWTSWVEjpa0iXNH9x43dQAk8doBmwUOtsyBYuzdyBXiMs2m396x1FMH9vOfd+z3FGvXzCRtx01neMOmVBIpmskv/rESWzdF5yzoLhQN7WihJJYy/iVXQd56tV9vP8NhxRcwf/67qWFx8+3y1o+tsJaOq/Dx5prdkZwUhjVmuaAXWd8cCDn+1k2gzcvnsZRs8fx2nkTOGbueBZM6WT8qFY+8IZ5gc1Z6smRs8YFLuGoeNBs6mGD0e8xEtV+Tom1jF/cYZUkHTmrWHLy1qNm8OI/vo20K077d6cfzt+dfrjv5Oy0S2xvSew1yZAwus1yU+fzht6BPG0xEWOHmy95g87zsUfd1MOB9vZ2du7cyaRJk4bkgjepGGPYuXMn7e3RE0kTK8bOYgwLJpe2ZUx7EqbC/mHSKeFf372Uo2eX15AqRUbZpU19dn/quFjGDiKCzgsxRwTymsCVdGbPnk1XVxfbt2+vfPAIp729ndmzwysx3CRWjNfv6GFcR0voyjxR8C4ur5Qzui1DT1+WXjse36GeBKVaNJt6WNDS0sL8+fObPYxhSWJn1fU79jN/8mh1lQwBo1sz9GXz9NhJXPWsx1ZGCuqmVpQwEivGew4MlCzxpzQOpyzs9ietpcnikk2tJAhN4FKUUBIrxgO5fCJ7PScRZ8Wrf7rjWUAtY6UGdNUmRQklsWrWn83Tmkns8BOFt2GKirFSPRozVpQwEqtmAzmjlvEQMbq1NM8vbtnUSgJQN7WihJJYNetTy3jIGOWxjFWMlapRN7WihJJYNRvI5WlNayb1UOC1jLVJilI96qZWlDASO6sO5NQyHio0ZqwMGnVTK0ooiVWz/qxmUw8Vo7wxYy1tUqpG64wVJYxEqlk+b8jmNYFrqPCuBa2WsVI1klItVpQQEqlm/Tkr9qRu6qFhlMcSbtfPXakWbYepKKEkclYdcMRYLeMhweuByOjnrtSEmsaKEkQiZ9X+rFrGzeLxL7252UNQkoikNIFLUUJI5KpNz2+11jLWmPHQ8YUzj+CIGWMZP0r7gSs1oG5qRQklcWr29KZ9nP+DPwNqGQ8lH37jAk48dHKzh6HUgIiMF5FbRORZEXlGRN4gIhNF5Pci8oK9neA6/koRWSciz4nI6XUaBeqmVpRgEqdmL+/aX7jdok0/FCUK3wF+Y4xZBCwFngGuAO4yxiwE7rLvIyKLgfOAJcAZwDUiMvj0eXVTK0ooiRPjvOv3rAlcihKOiIwFTgZ+BGCM6TfG7AHOBm6wD7sBOMe+fTZwozGmzxizHlgHHF+HgaibWlFCSJya5VxqrG5qRanIAmA78BMReUxEfigio4FpxpjNAPZ2qn38LOAV1/O77H2DRN3UihJG4tTsQH+2cFsTuBSlIhngWOD7xphjgP3YLukA/GI/vioqIheLyGoRWb19+/bwUWg7TEUJJXFq1t1bFGO1jBWlIl1AlzHmQfv+LVjivFVEZgDY222u4+e4nj8b2OR3YmPMdcaY5caY5VOmTAkfha7apCihJE7N3GKslrGihGOM2QK8IiKH27tOA54GVgIX2PsuAG6zb68EzhORNhGZDywEHhr8SDRmrChhJK7OuKevKMZtahkrShQ+AfxMRFqBl4APYV2I3ywiFwEvA+cCGGPWisjNWIKdBS41xuQGPQIRNYwVJYREifFALk/X7gOF+2oZK0pljDGPA8t9Hjot4PirgavrOgh1UytKKIlSs8/f+iS/Xbu1cF9jxoqSFNRNrShhJErNbl7dVXJfm34oSkIQNJtaUUJIlBiP8ayrq00/FCUpaJ2xooSRGDXL5Q192VI3l7qpFSUhaDtMRQklMWr26u6D9OfyXHrqawr70il1UytKItB2mIoSSmKyqTfvPQjA6xdM4vULJvHDe9czujUxw1eUEY66qRUljMSomdPsY1xHC0fPHs8bF1bo+KMoSnxQN7WihJIYN/W+3gEAxrS3NHkkiqJUjbqpFSWUxIixYxmPbU+MMa8oSgF1UytKGIkR430H1TJWlMSibmpFCSUxYtzdl6W9JaXlTIqSRHQJRUUJJTHK1t07oFaxoiQWdVMrShiJEeN9B7MaL1aUpKKWsaKEkhwxVstYUZKLrtqkKKEkSIyzjO1QMVaUxKKlTYoSSGLE2IoZq5taURKJuqkVJZTEiLEVM1bLWFESibqpFSWUxIhxd++AJnApSmLRDlyKEkYixLgvm6Mvm9eYsaIkFXVTK0ooiRBjpxWmxowVJaGom1pRQkmEGDutMDVmrChJRd3UihJGIsRYLWNFSTjqplaUUBIlxhozVpSkou0wFSWMRIhxcS1jtYwVJZGIPdWodawoviRCjLt7NWasKIlGxNqqGCuKL4kQ430HNWasKMnGFmN1VSuKL4kQ4+7eAVICo1tVjBUlkaibWlFCSYQY7+vN0tmWIZWSygcrihI/Coaxljcpih+JEOO+bI62lnSzh6EoSs2om1pRwkiEGGdzhha1ihUluWgCl6KEkgwxzhsy6UQMVVEUP5yYsVrGiuJLIhRuIJcnk1bLWFGSi2MZa8xYUfxIhBhbbupEDFVRFD/UTa0ooSRC4bL5PGmNGStKclE3taKEkggxHsgZWtRNrSgJRt3UihJGIsQ4pwlcipJs1E2tKKEkQuEGcnky6qZWlOSibmpFCSURYpzNG1rUMlaUBKOWsaKEkQiFy2ppk6IkG3VTK0ooiRDjgZxRN7WiJBpth6koYSRCjLP5PBmtM1aU5KKWsaKEkgiFy+aMuqkVJcmIljYpShjJEGNN4FKUhKNuakUJI5LCicgZIvKciKwTkSt8Hh8nIv8nIk+IyFoR+VA9B5nV0iZFSTZOaZO6qRXFl4piLCJp4N+BtwKLgfNFZLHnsEuBp40xS4EVwDdFpLVegxzQph+KkmzUTa0ooURRuOOBdcaYl4wx/cCNwNmeYwwwRkQE6AR2Adl6DVItY0VJOuqmVpQwoojxLOAV1/0ue5+b7wFHAJuAJ4FPGlO/S2BN4FKUhKPZ1IoSShQx9lNB7y/qdOBxYCawDPieiIwtO5HIxSKyWkRWb9++PfIgB/J5TeBSlCSj7TAVJZQoCtcFzHHdn41lAbv5EPC/xmIdsB5Y5D2RMeY6Y8xyY8zyKVOmRB5kLq9NPxQl2WjMWFHCiCLGDwMLRWS+nZR1HrDSc8zLwGkAIjINOBx4qR4DNMZYHbjUMlaU5KJuakUJJVPpAGNMVkQ+DvwWSAM/NsasFZGP2I9fC3wVuF5EnsS6BP6sMWZHPQaYy1s/XrWMFSXBqJtaUUKpKMYAxpjbgds9+6513d4EvKW+Q7PIOmKsCVyKUhMisgHoBnJA1hizXEQmAjcB84ANwLuNMbvt468ELrKPv8wY89s6jMLaqGWsKL7E3vc7kLNiTC3am1pRBsOpxphlxpjl9v0rgLuMMQuBu+z72D0EzgOWAGcA19i9BgaHuqkVJZTYK1w2p5axojSAs4Eb7Ns3AOe49t9ojOkzxqwH1mH1Ghgc6qZWlFBiL8YDecsy1gQuRakZA/xORB4RkYvtfdOMMZsB7O1Ue3+UvgJAjaWKmk2tKL5Eihk3EyeBq0UTuBSlVk40xmwSkanA70Xk2ZBjo/QVsHYacx1wHcDy5cvDTV51UytKKLE3Nx03dVrFWFFqwk6wxBizDbgVy+28VURmANjbbfbhUfoK1IC2w1SUMGIvxoUELnVTK0rViMhoERnj3MaqengKq1fABfZhFwC32bdXAueJSJuIzAcWAg8NfiC6apOihBF7N7WWNinKoJgG3Gqt4UIG+Lkx5jci8jBws4hchNW051wAu4fAzcDTWIu9XGqMyQ16FLpqk6KEEnsxdizjjJY2KUrVGGNeApb67N+J3TXP57GrgavrOxJ1UytKGLFXOCdm3KKWsaIkF3VTK0oo8RfjvCZwKUriUTe1ooQSfzHWBC5FGQaom1pRwoi9wmV1oQhFST7qplaUUGIvxv2OZZyJ/VAVRQlCm34oSiixV7i+AUuM21SMFSXBqJtaUcKIvcI5lnFbZvALxyiK0iTUMlaUUGIvxn0DVr8BtYwVJcGIWsaKEkbsFa4vq25qRUk+WtqkKGHEXuGKYqxuakVJLOqmVpRQYi/G/Y4Yt8R+qIqiBOGUNqmbWlF8ib3C9WWtmHGrNv1QlASjbmpFCSP2CteXzdOSFlLa9ENRkou6qRUllPiL8UBe48WKkng0m1pRwoi9GPfncppJrShJR9thKkoosVc5yzKO/TAVRQlDV21SlFBir3J92TytKsaKknDUTa0oYcRe5fqyOY0ZK0rSUTe1ooQSezHuz+a1xlhRko5mUytKKLFXub6sxowVJfmom1pRwoi9yo2omHE+B+vuVOtBGX6om1pRQom9yo2omPED34P/eic8/5tmj0RR6kvBMNZsakXxI/Zi3D+S3NS7XrK23ZubOw5FqTvqplaUMGKvciMqZuy48NSVpww3NIFLUUKJvcr1DYygmLGiDFd01SZFCSX2KjeiYsaKMmzRDlyKEkYCxHgkWsZqPSjDDHVTK0oosVe5bN7QMtLWMs72N3sEilJf1E2tKKHEXuVyeUNmpKxl7FgPAweaOw5FqTvqplaUMGItxsYYcnlDeqSIcS5rbQcONnccilJv1E2tKKHEWoxzeeuHO2Is46wtwirGynBD3dSKEkqsxThri3E6PULE2BFhdVMrww61jBUljFiL8YizjB0RVstYGW6om1pRQom1GBcs41Ssh1k/HBHOqhgrwwx1UytKKLFWObWMFWWYIJpNrShhxFqMs3nrhztisqkHeu2tirEyzCgsoahirCh+xFqMR55lrAlcynBFLWNFCSPWYpzNOTHjkSLG6qZWhikFy1hjxoriR6bZAwijYBmPuNImjxg/dwe0jYV5J9Z23p7tcO+/wLQjYf92OOEySGcgNwB/+AcYNQlOvGxwY1eUMNRNrSihxFqMR1Q29YFdxSzq/v2lj/33edb2qr21nfvFu+DBa4v3J70GFp8Nmx6DP33b2rf0fOicUtv5FaUSmsClKKHEWuVGVMx4yxprO3VJuRg75HO1ndtrabeMsrbdW1yv/0Rt51aUKGhpk6KEEmsxHlHZ1JttMT7kBBjYD/Z7p6+neMzOFyufJ9tXvJ3LWn/ufQDt461tz9by11eURqBuakUJJdZiPKIs461rYcxMGD/Xuu8kc7kFc+uT4edY/RP4h6nw+y9b1vU3XgNfmwv7t3kONMVzSwrGzoatT9XlbSiKL5rApSihxFqMizHjESDGvXth9GRoHW3d77ct4h6XkO7fEX6OrWut7aZHrUSt3j2Wlf3yg6XHOdZJ9xYYNdmKFffWGI9WlEhozFhRwoi1GBct41gPsz5keyHTDm1jrPtO3LjHFdc9uCf8HL32491biw1EAF7xirFjGW+DMdMg3VbuylaUeqK9qRUllFirXCLrjJ+7o7JoOjzzK+jrtm5ne6GlvWgZ93VbceN7/7V4/IZ7YUuIq9qxbnu2ljYOyQ+UHudYJz1boHM6ZFoh1x9tzIpSCxozVpRQYi3Giasz3rfJKkO65cLKx25/Hm56L9z2ceu+YxkX3NT74ZU/W1nW6VYYM8MS42tPCj6nI8a9e+Dgbuu2MwmOmuw60LZODuyyXONqGSuNRsVYUUKJtRgnLpvaKSHavb7ysf22Rbx7g7XN9kGmDVo77cf3w2a73OiTa6BjQuVzui1y57yd061tSwd88NfWbWdCHDhglTll2tQyVhqL1hkrSiixFuPEZVMX4mERxls41D62YBk7YtxtiXHnNBg7A9rH+byOh969lgUNLjG2G3lk2ihLohk4aIl0ulUtY6WxaJ2xooSSkA5cMRHjHS/AT98BoydZmcgfugMmzncdYE80EuUaxyPcBcvY5abe8iRMP9o+3DWJ/f6L0PUI7HnZKoXq2QIdE63t/FOge3PROh/tiHF7aXmJMWoZK0OHuqkVJZSEWMYxGeb252Dvy1Ybye7N8MSNpY87gilRLGPPpDRwEDIdpWK85xWYuMC6n3VlR9//b/Dy/bCvy9ruegleXW095hy/91VrWyLGLsvYOZ9axiMCEUmLyGMi8iv7/kQR+b2IvGBvJ7iOvVJE1onIcyJyen0GoGKsKGHEROX8iZ1lnM+W3s+0eQ6owgXnWKLitYxtN/X+7dC31yo9ch6PgtM0xGl1OXpy8XXcrkInvl2wjFWMhzmfBJ5x3b8CuMsYsxC4y76PiCwGzgOWAGcA14hIevAvrzFjRQkj1mKcsxO46hozzvbDq49U/7zurbDzhdJ96dbS+9XEjAvi6okZZ1ot0XzwP6z9ndOKj0fB5EDSluUORcs4ny11UxfE2LGMY+6m7t4SrR1oHFh3F2x8oNmjKCAis4EzgR+6dp8N3GDfvgE4x7X/RmNMnzFmPbAOOH7wg3D+9wZ9JkUZlsRajBtSZ/y7L8AP3mTFf6vhm4dZyw266e8pve9c9UeJGbst41zWEtFMu7WvY0Lx3E429NLzy88xZVH5vpnH2slexhpHx0Rrfz5LiXXiFuMkWMa//xL8/N3NHkVldr4I//UO+MkZVulYPPg28BnAbZZOM8ZsBrC3U+39s4BXXMd12fsGh7qpFSWUWCdwNaTOuOshaxu1MUcYZS0kq4gZuy1jx+p13N6X3AvfWmzddtzUJ/8dnPBxS0RN3nIv73gefnCq9fjnNln728ZYYnxwl3VMm+32zudKuyA5TUFaOqw6Y5O3LgrSMf2X2L8ddq6zvreO8c0eTTB7XTq2ZQ0sWNG0oQCIyNuBbcaYR0QkymD8/nl97VkRuRi4GGDu3LmVBmKfScVYUfyIt2XciJixE/dN1SEM5hX0whKHEcbrtozdyVQA41yGiOOmTqWs5K7Rk6FzqiWyThkTWI85rTQdsWrpKMag89nSCbHEMrbd7XG2jp32oGEdyOJAd+xWwjoR+H8isgG4EXiTiPwXsFVEZgDYW6cJehcwx/X82cAmvxMbY64zxiw3xiyfMqXCWtgqxooSSqzFuCHZ1M7ShPWYFLyWsSP0UdzU7oQsr2XspqRzlofRAROgU5Pc4srOdseMMZB1JXCl7dcdOAi3fhS2xHAFJ0eMnUYoccXpJd42rnSsT94C932reH/dnXDHFQ0fjjHmSmPMbGPMPKzErD8YY94HrAQusA+7ALjNvr0SOE9E2kRkPrAQeKgug5EUGjRWFH9i6pO0aIxlbPdpjpoQ5ceJfwN/+k5xYQaHQsw4wjlybje1fduJGYNVw7zxT5ZFHEQ6A2/6Asw7uXS/s15xxmsZu+J2fpbx1qfgiZ9bCW4fr8/8WzecHt5bYmFtBtOzzbrAOeSE0rH+4iJre9Ll1va/3mltT/kMjJo4tGO0+Bpws4hcBLwMnAtgjFkrIjcDTwNZ4FJjTC74NFUgKbWMFSWAWItxQ7KpHet1YBBifPKnrQSwPRs9565izsr6uKndlvEhJ1h/Fcfyd+X7fC3jHKUJXE7MeFTxIuDATmvrxJnjRMEyjrkYd2+xQgszlsLzv7HG7XwHfgxhXNkYcw9wj317J3BawHFXA1fXfQAqxooSSKzd1A2NGQ/GMk612ElSeyxR7dlu7TfVxIz9ErjaAw+vikLMeJT1BxVKm+yLACf7N0w8mkX/fkBgx3PQf6Di4VXRsw1yA5WPi3SurbYYHw2Y4hrTDvk8bHu2eD/uFxd1RVSMFSWAWItxLteA3tSO9TooMc5Y5UcHd8MvPwL/cqglcFXFjF11vQN1FmMnsatjQjGp6zWnBSRwjSq6qQtiHDPLOJe1YtzTjrTGvuP5+p27fz/8y0K447P1Od/+7VaC3bQjrfteMe7bBz9+S/F+3N3u9URSup6xogQQazd1Yy3jQWQOp9LWAgwD++GpX9jnzRWFPkppk2MZm1z9LePjPgRTF8O0JZbL+ZNPWALtLB6BT2kTFN3UcRPjAdtFPXEebH3SKtuqF857fv639Tnfwd3WRVDn1OJ9Nz3brMS/Q98MJ14GYwdfwpsY1E2tKIHEWoxzeUM6JUgUcYvKYN3UkrLE1ik5cjC56tzUzsVArt+VwOWTTV0LLe2w4JTi/Qnz7GG53dTOBYArgWu/Xd0St5hxn90AxRGusvruOpzbKSsbLL17rTBBpt3qbOZN8nO6uB1xFsw/2fvs4Y1axooSSKzd1FlbjOtKrg6WMZSLcYllXEUHrk2PwZqbrNv1sowD8SRwpdusbG3HMl57q7WNU8zYGKv7FsDYmda2rmK8z9q2jhr8uQZ6rYu89nHWBVv7eGusbgG68a+s7Zjpg3+9pCEaM1aUIGItxrl8vv5rGRcs44ODO493MjX56tzU7ouBp39pbZ34bqPwduByrMF6WeSNYPd6eOoW67ZjGdeje5qDc66WOlyAOBcJTmlZIcnP58LPcWOPJETQOmNF8SfWYtwQy7hQZ1xny7jETR0Bv/WDGz1BuxO4+nqK4u9d8KKaEq1G48427pxqJc/V0zJ2zlUPb4DjknZKyzrGW+d3yrLcdI5Ey1hjxooSRKzFOJc39bWMcwNFEYwSMzam2LHLS8dESxgc8jnXEotVWsZgTeDplsrPGwzuDlz9PcVELa9l7F0qspm8dHfxdsto2/W7p7ZzGVN+oeGcq6Vj8PFMP8u4d2/5giIQ3D1tWKNuakUJItZibFnGdRyie1KMYhk/eC38/YTS1XecCTuVgtEuS7bETV1FzNihHkvGVsRlGbubUXgvAuJiGT+9Eh65vni/pb0ocLXwmyvh7yeWiq5zrmdWwlfG1zrS0nM5dd7t4yyx9xPjuC7I0UjUMlaUQGI9I+RyDbCMHaJYxo/+1Nrue9X/cbeI5XOudpg1WMZDEUtzZ1P39xTFeOIC+MBtluv9mtfHxzLeuc7aXvhbK8Y9bUkxDlsLD37f2joZz9CY+LPjpnYSuBw39YorYfHZ/iGKkYBmUytKILEW47rHjN1iHKUdplu8Crhuu0XXuLKpq+rANYS4e1P37y9dhMJpyTh2dnws4969Vjx7zuuKn7UThx0MPVuLYlzX+PMea+t1Uzt9tResgKlH1O/1koZaxooSSKzd1Ll8vjGLREA0y9h56cAJxDU2d8w4yDLevRGethfHyXqtozonqvnhTuDq7/GvJ06l42MZ9+4tlgk5OK7fwdDjWuZwsOdyUxDjsda2Y7z1Wa652bofp5KxZqClTYoSSLzF2NS5+1bOJTJRYsZuS9L3cY9lXKnpx6M3wC0XFd3Ebt72jcrjGSzuBK6+Hn9xiJUY7ylamQ6tnf7ZydXgXnPYL55bK7177SYqdkLcjGXWds2N1rZtbP1eK4noEoqKEkisxThvDHWtbKrWMnZENXARAa9lXKHOODdgjSE3YK3u4/D+X8JR74ownsESkMDlJpWprkSrkTiWsZt0S22LOrhd7z2uz75eC0SAFTN23N9gdUE79gPF+95yuJGGxowVJZBYi7ExhlQ9W2GWJHBVYRkPBFhi7qxpv7IZL46F3dcNB3YU97fUoftTFJzx5nNW05NWnyYjqUx8LGOvuEHt49vv+ry7GyTGfhcPTj1x6xgrG3wko25qRQkkkhiLyBki8pyIrBORKwKOWSEij4vIWhH5Yz0Gl89TXzEusYxDOnA99jP4+0nFST/ILRrkpg4qbXLE2rsOsldwGoUzrkILyAA3dW4Arp4JD/1gaMbl8PAP4bvHFK0nP3FLtVQW41suhB+9pXSfO07cs6142y+z+efnwSM3RB+3g59bfYxtDaeGonQt7qgYK0oQFbOpRSQN/DvwZqALeFhEVhpjnnYdMx64BjjDGPOyiNSllVTOmEhVQtFPaE/iqZZwK/YPX7Um/P32OsWB6+dW6aZ2xHr3emt79jWW23XK4ZGGP2iccTnZvX5iLGnr4mNgP9z+aTj+r4dmbAC//ltru3sDTJzvL25RYtrOSlpuBlwXX+4LMb9zbbgPxtWwmlLv3vLOWgXXtLpn1U2tKMFEsYyPB9YZY14yxvQDNwJne475K+B/jTEvAxhjtlEHjKlzaZNjGadbCZ0cOyZYW2d5vaAkn2oTuByrYJctxlMWwdHvrjTq+lEQY/v9+C2VmMoUl1dsFlvWWJN2XWPGrue4E/n8LONsb22iEeamVrS0SVFCiCLGs4BXXPe77H1uDgMmiMg9IvKIiHyAOpA39XZT25NwuiXcUHGsMWeiDhSnCqVNe14p7a3sWM7OusJjhjqhx2MZ+5Y2ZQafrVwLz91RvP3SPZaLPJ/1jxmbXPVi6RZdtzXsFfZ83hbuGsTYL8Y9EheECELFWFECidL0w08NvTNVBjgOOA3oAB4QkT8bY54vOZHIxcDFAHPnzq34wnXPpnYsoqiWsUPUmLHTx9qJzX77SGt71d7iMQB7u6ztqEmRhl03nHE5lr5f4lgqU7Sch5L/+5vi7dU/Lt6edGjpcSm761k+W10vb+e7l5THSvaIsdOMpVqxz+etWLzXMnZW9zrxb6o733BEE7gUJZAoYtwFzHHdnw1s8jlmhzFmP7BfRFYBS4ESMTbGXAdcB7B8+fKKs10ub5BGJHClW8MnW++EGmgpui3jfOWSIEesneYQ6SFeutArxkEJXPWsvY1K9iAcfZ41YT/x3zDlCLjwNz6WsZ0IVbUY25Zxy6jSfIG8R4ydkrdqRaO/23qON8adaStejI10tM5YUQKJ4qZ+GFgoIvNFpBU4D1jpOeY24I0ikhGRUcDrgGcGOzhT96Yfjhi3ED4peB6LbBnb1pdX6B0Rdib43n2WBVrPRTCi4IzXeT/OesZuUunS9ztUVnI+b3kKph9t3R87wz/L3BHgauPGjui2dHjc1J6YcaEzWpWiUVixaVz4cSMZdVMrSiAV1cAYkwU+DvwWS2BvNsasFZGPiMhH7GOeAX4DrAEeAn5ojHlqsINrWNOPSpaxtwY5cszYtri8E06fx03du3forWJwWcb2+8n41L2mMqXW4i8/2vhxgSWK6RaY9BrrftASg86yld97bdHdH+n8LjHOBSRzQTHTulo3tYpxZTSbWlECibRQhDHmduB2z75rPfe/AdS1p2Pd3dTumLHXPenGK8aRsqldbmqvGPfuteLQjlj37vHPZG44Xss4IGbs5uU/N3ZIYE3Qjhi/5k3wxr+F1wVcBDjj69liLa/4pi9Ee42CGI8Kt4wLJVBViobzP+PnbVBsNGasKEHEvAMXpBsSM24Jv0L3rqgUVGcctGqTnxg7x4Ad72yNNuZ64u0oFuSmdpj3xmKDkEaSzwHG+kzSLXDal6CzgmUMxWSuKBRixh43tfeizPmuqzXgKtWYK3YCl1rGiuJHrMU4b0x9w6q5iHXGZZZxhASugV7YYeerOROOIxzOOrdukW6mGIdZxuIS486pVkJTlOUmB4P7IqkS7mOq6WrlCLDbMnavQe3ghCSqteCMJ5NeKUdjxooSSKzXM87Xuzd1SZ1xmGXscV1GSeD69adg36vWbWfCaR1tWcWOZezO4q0mE7heOOPN9lqi6zcGt+XpNKzo3dvYvsrO5x3lAsU9vmouaNyWsXNR5pcEVsgPqNKCK4ixtr0MRMVYUQKJ9WV8zlDnmHGNlnHQQhFuy9gRYii6o52FGJxSprhYxmBZiH6fbYkY2w0rehtcmlPyvVSgRIyrcVP7ZFP75Q0ULONqxbhCX3JF64wVJYRYzxzGGNINyaZugGXsxplwnA5XfpZxphli7BpvUKKRW+ychhXOxUSjcD5vb/KYHyUx4yocO34JXH6Wcf9gLeNY/6Sai9YZK0ogsZ45GuqmripmHKG0yY0zMTuLzMclZgwUxhwoxu6Ysd2us+GWcRVu6pKYcRVi7FyIZdrDxbhmy9j+bnV1pmDUTa0ogcRbjPP1dlO7Spuqsozt0qb5J8OFvy3ur2QZO9tCVyd3zLhJYuxYbkFrKPuJsXMx0ShK2pRWoGbLuN/KvnYvNOG3SEStCVzeVqhKOSrGihJIAhK46nnCAUDsxQZCJgWvZexY0af/E0w/srg7aOJ1zu24pb1baE4CF9hxO4ITshyBkzSMnmzdHio3dZTPZDAx43SL3dTE+T4a4aZWyzgYLW1SlCBifRmfr/cSis6EjBA62eb6/CfVMkvMNTb38V4x9msGElvL2H6PmXZoG2vdrqcY9+4tWpEOtWZTVxszLohxhGzqINEwBg7u9tnviLHWGQeiHbgUJZCYi3EDllBMtRStwyCy/f5tDb1LDrrH5m4t6Uw47npWiIcYR40ZZ9os6znTbvXSrge9e+Frc+Gefyzdn6u1zrjKmHHKEWNPzHjq4uJxlUqb/vRt+Od50L21dL9mU1dGs6kVJZBYzxz5vKmvoZEbgHSGSJaxs0jBtKPg3Bvggl/BuNmeA91i7Oo1nXd12gJX5604uKkdyzhAjB0L37m4yLT5x1Zr4cBOa/voT0v31+qmroZcf7HDlzdmfOrn4fWXWrf7K1jGf/qOtfUmtWkCV2VUjBUlkNjHjOvqpnaso7C2fLls6VJ4Y6bBknP8j3VfKbgt3aCYcUkCVxMWioDobmonppxu9Ymh14hjYWc9Hb3yNdYZVxPXzWWtC7FUxnpePl+8WMq0w6xjrdtOTXmQaDguam/LVC1tqowmcClKILGeOerupo4SM3Ym2XY7XprP+h8HlJY2uc5XEOOs534csqkrualtsXMuFtKt1S9XGIRjTXrFvdamH9VM7I5l7Dw/ny11jzsWbVgClzvW7X0PeXVTV0TrjBUlkFjPHHlTZzd1IWZMsGVcWH1ntP2cnP9xUGoZuwWrUNrktYzdMeMmu6mDLPOCZWyLdbqlfm5qJxHMaxlX46Z2H1NNMpA7Zuzcd7+u454fCFlC0Z3I5n0Pmk1dGbWMFSWQeItxvs5NP6LEjJ0JunVU8TlRcIt2mWXs56ZucgJXUOzVsRCdsqZ0HWPGhTir57MvdOBqZMzY9oo4Yp7PuprAtBbfd2DrU0q9JGVibL8ntYxD0JixogQR65kj34glFCvFjAuWsS3GYW5q99jyPpax1yLOx8EydsQ44Kt3RGmUI8YNcFNDsdEH1NdNHZgL4Cptcl7f3YZTPG5qv3O7L7i8bupCNrWWNgWipU2KEkjMxbjeSyhm7ck4gmXs9GWeMC/4fG4ryC1Y+RxcNa64FrBvAleTY8ZB7lTn4mP0JGubbilPVqoVdyevAzuKt6spbSoRY7+4bkBYITRm7LaMQ9zUoZaxZlNXRN3UihJIzLOp69wOM29n1EaxjKctgb+6GQ45MeSEbsvYnqhnHQe7N5QeFsemH0GicWCXtR3liHFrA9zUFEUPau9N7WsZB4hx3r4QK4kZuy4CnM+l4Kb2+f9wn9u7xrMmcFVGLWNFCSTWM0dD2mGmImZTp9vgsNPLG324EU82dcsoq4FEWaatTzZ1M1ZtAirGjJ1a4IKbuqWObuo9xdvuc9bqpvbNeK5gGZfEjH2yqUMtY7ebOiiBK9Y/qeaidcaKEkisZ468MfWNGTtxw1DL2LbSIomld2xiTepB8USTKzbTaFoCl02Qm3q/7T4e7Y4Z18Ey7tkOT/5P8b47xl6wjKtcQtE3rhsQ4w+NGbuzqcNKm1xi/MzK0i5cmk1dGanQbEdRRjDxFuO8qbObOhchZuyyjCvhHZukrD/vAgSF5h95aLUt7WaJcSG2GfDVL/+QtZ213NrWqwPXEz+3th0Tra37nLX2pva7oApyU3vF2J1NncqUu+0rCf1L98DP3ln+umoZB6MxY0UJJNYzR92bfpi8JaCRLOMoHbK8Yiz+k7E7Zuy4vZuVTV3Jglt0Jly1FzqnWPfr5abet9ly47/jB9Z9dza1u8SoEpVixt5FKAr7feqMnf+BVLr884gi9Ls3lh+vYhyMirGiBBLrmaPuMWMMloBGsYwjCEOZZRwgxu5s6tYx0c/fCAoCFDF3r17tMHu2wJgZRTH1WsaSipaJXDFmHOSm9smmdsd5y147wrndXcw0mzoCGjNWlCBink1d597Uxtiu5AjZ1DVZxqkAy9iVwNXmiHGTelPjsgajMNg64zX/Aw9+3zrPmOmuBCpPzDjqxYl73NVkUzu9qQsXA24x9rmI8k3g8ryee6WuvNYZV0SzqRUlkJiLcZ1Lmxw3dZQ640iWsVd4g9zUrvaYUxfBEWdZmdrNoOBOjSrGg2yH+b8ftrYdE2HBKcXP1ZtNHaX7lpdq6oyzvZDpKIp5XSxj12IbmsBVGXVTK0og8RbjfAPc1AXLOOAQp2QlimUclMDlxd2JK5WBN3ws8ojrTrXu1Hq1wzy4Czqnu7KZvZZxLTH0KhK4Bg5abuWUq7TJLcZlMeMIVneLew1rLW2qiIqxogQS65mj/m7qPEXrtUICVyQ3sl/M2Ge8xpVN3ezJumoxHkQC1ysPld4fM81lGbtjxgO1xdCjWsb5PGQPWpZsSQKX2zL2XJdW6sAFHstYs6krEhYeUpQRTqxnjvq7qU3RTR10he4kcEWpM/a1jH1Ezp3A1Ww3ZrXu1HRr7e0wb7mo9P6YmaVNNxwaJcbO4463o6Xd9fq5Cm5qH7xCn/GxjGOWwCUi7SLykIg8ISJrReQr9v6JIvJ7EXnB3k5wPedKEVknIs+JSP3iKbqEoqIEElsxNvZE2jg3dT0sYw+BCVyupRTr2my7FmpI4Mpng0uGwji4C15zWvH+9CNdbmpPNnU1buq/fc7aVnIlO9+x01WrZVTxfecGSltYRkrgCllOM75u6j7gTcaYpcAy4AwReT1wBXCXMWYhcJd9HxFZDJwHLAHOAK4RqdMVpHbgUpRAYjdzOOTyjhg3wE1t3Ql4Yae0KYI4lI0tqLTJSeDKx8cyjlza5JP9HIV8Dvp7YPZri/smHx6QwFWlGBfi+ZUE0xFju6tWWczYVRscJYHLGzN2X1Dk4ynGxqLHvtti/xngbOAGe/8NwDn27bOBG40xfcaY9cA64Pi6DEZjxooSSLxmDhe2Ftc5Zky00qZMe8QSlailTW43dUxixtW4qaH6JC5nUYiO8a5zZQLqjAeqTOCyP/dKXbLK3NRBMWO/ph8RWm2666/jaxkjImkReRzYBvzeGPMgMM0YsxnA3k61D58FvOJ6epe9z++8F4vIahFZvX379igjUTFWlADiN3PY5O2JtK5lm1FLm6K6qCM1/RBXO8xc82OKhaYfEb96xwqtNonLEeP2cXD2v8N7fmbd94sZ56uMGTufcaUuWc7EX2IZO2LsjhmLTzvMEKv7TV+wtjmvGAck8DUZY0zOGLMMmA0cLyJHhhzu9wZ8fyzGmOuMMcuNMcunTJlSeSBaZ6wogcS2tClvGuCmduaUipZxVGGIIMaZNksgjLFev9lu6kLMuEo3ddWW8R5r2z4eFr2tuD8V0IGrKjEOs4zd+7wx447iRUhBjG0BLfteQrKpF70dNj8BO9a5Do+B16MCxpg9InIPVix4q4jMMMZsFpEZWFYzWJbwHNfTZgOb6jIAdVMrSiCxnT0cN3VdE7icDlx1s4wjNP1It1oTUNyybat1U0dtibnnZWvrtoz9zudt+lGNm7rwGVcoPyqzjN1uarvOOGh9Z1+r2xVvT7eVLqNo8vH5bl2IyBQRGW/f7gD+AngWWAlcYB92AXCbfXslcJ6ItInIfGAh4KlRq3UwKsaKEsTIsowjLRRRhWXsV9rkdf+mWy0rLG7tEqvJpoZoburHfga3fQwu/B0c3GPtc8eMwWVpexK43DW7FQmxjMOyqTPtxYsQkysV47ILqxChl1T5alb52FrGM4Ab7IzoFHCzMeZXIvIAcLOIXAS8DJwLYIxZKyI3A08DWeBSY4I6qVSJZlMrSiCxFeOCEVJvN3VFy7ivirKmCAlcqYxtGTtiHBPrqZp2mBDNTf3iH6zt7vVFS9prGYtYn0mtvamdc0CEOmPHMnaXNnlixoGWsZ8L3D53KlO+gIb7XDHCGLMGOMZn/07gtPJngDHmauDqug9G64wVJZD4zR42uUbUGbtjhGF1xjVbxj5uapHSZKG4uDKrtowjiHH/fmvbMqo0Zlz22p5+17ls/dzUplJpk9Ob2msZuz6PdFt4B65U2rKy3e/BxCEfIOZoApeiBBJbMXbc1A1ZtWkoLWOwBCIfM8u4mt7UEM1N3W+Xsx7YAb//knW7dbTPOVvh/n+Dbc/Y567SMg4tbfJzU7tKmwqZ2CGWcUsHoUKfylgXbCWWcWzd1PFBY8aKEkhsZ49iadNQx4z7Iy6fSBVNP3LErndx1IuCtk5r6yRkheFYxi/90drOWOYfI3dc1DecZW2rbYdZEFSfx3zd1H6lTVnXxRmln0fntHAXuKTtBTT6iscVyuaUUFSMFcWXmChDOYVy2LqXNlWqM+6rQhgiWMbG2JZxQt3UnXYviJ6tlY91LON9r1rbc77vf5wjjo645fqthiBRCStt8nVTuxK4gtzU7s9jwjx8/z8KMeN0MZTheAximk0dK9RNrSiBxFaMi+0w63hSxxLytdbsWuDBWMaBSyi6SpviYhlHrTPunG5te7ZUPrbPFuO9thh7k7e8OBnUVSdwRS1tcsWMM+1WprvzvgtuavGck2B3qjtmXHDf267q+GZTxwd1UytKILHNpi6UNtU9ZuxYxu77wN9PhIWnV2cZ+yVrefeNmQY7X3TFG2NiPUV1U7eOgrax0LMt/Lh8zooVA3TbPSK8ZU1eWjrs52br2PQjoLTJeS0JSuCyzzlxAYHrXZfEjG0xzvZDmz2WuOQDxBUtbVKUQGJ7Kd8wN7XbMva6zF74bZVWWoVs6gv+D+a90ZqA8jGLGVezelTnVOiuYBnveKE0uziVqVw77AhktQtFABAQ9y+Z7N1ibI8lyE0N8Nd/gL++O7gEp1BnnHZlmfcVXzcu321c0dImRQkktrNHY9zUzqpNISs3VVNmUzY2jxjPP9ma/EsSuGJiPUV1U4Plqq5kGW9ZU3q/fXzlhCa3mzpVpRgHWVl+CVxZt2Vsf0febGqAWccVrXnfBC5XB66CZaxiHBl1UytKILGdPRrTgcvYWhzSNKIqKy1CzFjSxYkfkuemBsvVXilmvPkJKy47ZZF1v1K8GCwXuGOhVlXaRGXrFfzd1GC9d287zJJzByT4lcSMPfXXJh+DtapjjoqxogQS29mj0Ju6vqaxJ4HLb8IdiG6l+SVwtY8t3edYxkmtMwYYNQkO7Ao/Zs9GKwvZEeFK8WKATEdRzGpyU1dqh+kqbXK7zFOZorfC15oNObfz/+PtTKaWcQRCLoIVZYQT29nDNLIDV1jTiKpqXn1ixtOO8uxLAyZ+MeNqLgoy7ZUXinDWgXaafESxjE2+WBpUtWVcbczYZRkXQgcm2DIO6sDlfG7eZRw1m7oyYUtfKsoIJ7azR65hbuqQBC6wxThiPNXPMp64oHRfIWHIFp24uDKrsYwz7dYKRWGTaLbPiqMWxHh85fPm+gchxkFuar9s6gOWFe6QcoUO/L6PsHMXYu0e74pmU1emIMbqqlYULzFRhnIKuTKN6MAVmsA1iGxqpHxydyYgx50Zlwm7KjFuA0x4S0znc2tz3NQTKp831+9yU1dbZRfkSs6X3/aNGfskcJWcO0iMHcvYc0GnburKhJWkKcoIJ/51xnXtMGh34AqzjDGDixkDXHJvscuUM3nnsqX3m021bmqwrOOgRTSyfTBqNJzwCRg3G446N/h8H7kPfvhmS9wLYlyDZVxp1Sa/0iaw3dQ1JHAZlxiXWcbqpq6IirGiBBJbMW5InXHJQhEQWPMYubTJp+kHwIyjXfscMe73f06zqOaioMUlxoz1PybXb3WlmroIpl4Zfr7pR8G8k+DAzsbGjIMs41TGv7SpePLgDlxBMWNj4nOhFVfUTa0ogcREGcopxIzrOUL3QhEQHAMdTGlT2SFxdVNXcR3mtoyDyPZFX3oSLPHNDRRj6dVmUwfFdY1fzLhKN3VgApcrZuzrptaFIkJx/vdL+ocrigIxFuOG1BkT1TKudT1jn4+zkMAVNzd1FV99QYxDMqqrWnoSS3xLYsZVWsaB1quntCmft5t+eN3UYWIcUsMc5KbWbOrKOJ+dWsaKUkZsZw/TqGzqijFjqrAaI4yt4Ka2LcC4TNhVJ3BRwTLur8Ey7it+LjV14KpgGUNxzN7Spopu6gAXeKhlHJMLrbjifNZ5tYwVxUtMlKGcXEOyqZthGXvd1DH5yGtxUw+EiHHVlnGrJ4GrTu0wS2LGprh8om8HrlrqjD0LS2gCV3RELWNFCSK2s0dDsqkbHjP2GazXMo6Nm7rGbOogqll6Enzc1DV04PJ1JXsSuJysdm8CV8FN7fcPFnRunzpjR1i0tKkyKU3gUpQgYjt7NGQJxcIE67VqPBNv5GzqKmLGsUvgqkWMQ2LG2d7q4r6ZNuszKViuFVZ48hLU59h4SpsKbupqYsYhlnFYnXFcLrTiirqpFSWQkVnaVDaReibeyPHLKNnU3g5cMZmwq7KMK8SMnT7PVVvGA9C/37rf2hn9uRCe8exg8rXFjEPrjAM6cOXVMq6IuqkVJZDYzh4NW0KxpAOXa7+bWmPGfgldqbgmcNWSTR0gxo7FXI1lnG61LOP+Huu+00YzKoGlTVFjxjV24JIQyzgu321cKdQZq2WsKF5iO3s0zE3taxl7xXgQvanLjompm7oaKlnGub7S46KQbrXcvn3d1v22Ki3jKKs2YeDgbutm65ji7igduILKpgpuaud5KsaRKZT5qRgripcR5qb2rNpEkBgPYtWmskPsfbmYLRRRDY5VGWgZ11Ar7MTlnaUZW2qwjKO4qbc8Zd2ecnhxv9OBC5/1p62TE7nOuJDApdnUFVE3taIEElsxbkw2tQnIpq4xZlxV04+YuamroWAZByRw1WoZg2W5ZtqrXygiUjtMA5sfh4mvKV1n2nFTp6ghgSukzjgu+QBxRdthKkogsVWGYsy4ER24XPehfpaxX8y4rOlHAifsijFjxzKuUYyrTd6CiDHjPGxZU9orHIrZ1EFJV2HnFo9lrG7q6DheIXVTK0oZsZ098o10UzcsZhyWwNVfej9JOMIZ1PSjYBnX4KY+uKv65C0gUjtMDHRvgfFzS4+ptQNXSWmTZ6EIbYdZGXVTK0ogsZ09Cu0w6zVCZ9L068BVt5hxWAJXgi1jEcs6rphNXY1lbB9bs2UcoR1mPmddBHnj0alMjQlc2eJFhHc5QGOS+d0OJZpNrSiBxFaM504axYdOnMfEUdUuIBBAQYylPHblnXhrjhknqLSpWjLtITFj2+qvtjc1wIHdNWRSE57x7ODUMLvLmiBaaVNgApenzriv2/puddWmymg2taIEEtsEriUzx7Fk5rg6ntHVfatS0496LqFYEGNbyJqdTT1mBnRvrv55mbb6WsbOGsk9W2DKYdWPJ0gw3QLt1woTBtmBy5PA9T8XwGFv1WzqKKibWlECia0Y151QN3Wt7TC9k6+PZeQkPzlWWrNdmR97AHr3Vf+8oPaT4LKMqxDjybYA57O1xYyDSpvcYyxYxp5WmxU7cAUtoZgrL20CeP4O6/0kMR9gKNFsakUJZASJsT0BCBESuOq4apNjlTnC0OwJu2OC9VctQeIHtXXgmnSoJZIDB0obckQeT4Pd1H5vtcQy9jxPs6kro9nUihLIyJg9jIF7v2ndjpLAVet6xr5ibFtlTqepxE7YAW5hqK3OOJWGaUus27VaxkH9ox0CLeNBJHAVxNj7mLqpK6KWsaIEMjIs45f/DH/8mn0nQtOPmi1jHzd1wTK2ezA3201dK2GWsV//5ygc/R7o3grzTqxlQAHtMN0x4wDLuOCmDljPuJoELvfrJvW7HSoKMWO1jBXFy8gQY3fiUaTSpjomcDlWWVzc1LUiBFs0B/dY2/bx1Z3z+L+2/moaT4R2mP0BCVwlbuqAFqZ+5875LKHooG7qyqQ0gUtRghgZs4f7x+/XDrNWN3WUUhZvzDipE3aQWxigdy8g0DbW//GGjCfCQhFBMeMo2dSVLGO/mHGzM+Xjjq5nrCiBjIzZo8TKqbBQRKqlinrRCMdlbCHoc9zUSf3IA8QPoHeP1ft5KMUo6HPM+7mpq8ymDu3A1VI8xk1hERIlEHVTK0ogSVWG6iixjP2WUHRNvNW4kb1zr98EnkpZ5U393dWfP04EuW7BsoyrdVEPmggx4yA3dc0JXLnyOuPCY9kqwhsjlIKbOuD/SFFGMCNEjF1X4uK2jJ3HPWIdmTI19j+spcNVWpVUMQ5xUx/cA+31bNASgSjtMIOafkjasqBr6sDlU2dceGxkpGDUjHMBo25qRSljhIhxJcu4RjGOemzGJQZJtYz9LNEnb7Gs4t690DF+iIfjGc+BXfD0bQF1xj5u6oqWcZUxY7fVrPijbmpFCWTkibFvzNg18VZjuZZl1IZYxoXnJPQj92Yv73wRfnER3PpRO2Y8xJax13p98n/g5g/YyWQ2/futcXtL1WruwOXTDrPksaReaA0Rmk2tKIGMjEv5StnU7om3qmb/Ud3ULssssW5qjyXquIC3P2vdHuqYsffiwClfy/YVY8IDByyvRFk9uJNNHVJn7L2wMsYS8KA6Y3VTV0azqRUlkISaaVUS6IYerJu6Bss4seUvHkvU6W+d7WtizNj1veWz9nagKIr9+/0bkRQSuAK6ZvklcDnnD7WMVYxDUTe1ogSSVGWoDu/EWq+YcTUJXJBcqxjKLdHePda2bx9kDzbHMnZ/3o61lXOJ8cCBADFOW8/NZwM8IT4JXAUxDkjgMnkV40poNrWiBBJJeUTkDBF5TkTWicgVIce9VkRyIvKu+g2xDuS9YhtSZzwYyzgIx02d1HgxlCc1ObHZPttCdpZEHLoBhVjG9qQ/0OvfL9u5KMoN+Md5/b5X5/xO+VLYcpmKP+qmVpRAKqqDiKSBfwfeCiwGzheRxQHH/TPw23oPctBU7MDljhkPwjKu5KZO8mTt7UrltMB0SA1xja3XUnfEMudyF+f6/MdVWOQ+G5LAhf/5g9zU7scUf3ShCEUJJIryHA+sM8a8ZIzpB24EzvY57hPAL4BtdRxffaiYTV0vy7hCAleS3dReS9SdtQyQHmIhKosZ29aWO2Yc1Igj5bKMA+uM8Yixff4wwR3qC5KkURBjtYwVxUsU5ZkFvOK632XvKyAis4C/BK6t39DqSEnTj1S55VPSDrOa0iZvf+IgMW6v/txxo8xNvaf08WZYxm4KlvFA6Vh8xdixnPuDE7jA3w0etFCE+7yKPwWPhIqxoniJIsZ+gVGv6nwb+Kwx4Ze8InKxiKwWkdXbt2+POMQ6kBtwD6J8svW6sSMT8VhHHKo6d8zwJkyVWcZDbRUGWMbeOLDfcpjurN4wy5gwN7XGjKtGtM5YUYKIIsZdwBzX/dnAJs8xy4EbRWQD8C7gGhE5x3siY8x1xpjlxpjlU6ZMqW3EteBMpOCfwEWNMeOCuPpM3m4KST9Jnqw94tf0mLH4x3S9JUZ+1qpbNH0tY3sbFjP2uxCLoWUsInNE5G4ReUZE1orIJ+39E0Xk9yLygr2d4HrOlXay5nMicnr9BqNuakUJIoryPAwsFJH5ItIKnAesdB9gjJlvjJlnjJkH3AJ8zBjzy3oPtmbcYkwDErgqlWw4Ypxky6mstClmMWPjEzMGf8u4ohg7+0JixslxU2eBvzXGHAG8HrjUTsC8ArjLGLMQuMu+j/3YecAS4AysC+v6/OMW3NRqGSuKl4rKY4zJAh/HypJ+BrjZGLNWRD4iIh9p9ADrQiXLeLAJXJWeUxCEJLupPeLndLxyGPLkJU92t/Mde+t9/dznbm0JrDOm9OLDCXUE1RlDLMXYGLPZGPOofbsb6zc8CysJ8wb7sBuAc+zbZwM3GmP6jDHrgXVYSZyDR7OpFSWQSLOHMeZ24HbPPt9kLWPMBwc/rDoTGDMebNMP73MCLGNnkk70JOQRP+97GeqYcVlpk8v1WRIzDkngcs5Tdu6wBK6wOuP4ibEbEZkHHAM8CEwzxmwGS7BFZKp92Czgz66nlSVs1j4AdVMrShAJ7kIRQvfW4lq2uzeUu6lDF4qoxTKu5Ka2LeMkT0Je8TP5Umt4qIUoqB2mdyxhdcYwiAQuP8s4vmEIEenEKj38G2PMvrBDffb5/mNXnZCp2dSKEsjwFONvHgbXnwmvPAzfWQoP/6j4WMkSiva+wbbDrOimtgUhybGyshhtHjKurltDLsbedpguMXZbw74x46iWcVidcTLc1AAi0oIlxD8zxvyvvXuriMywH59BsT9AlIRNoIaETM2mVpRAhqcYA2x6FLY/Y93u2VLcL5WaftSwhKJfxyY3BTd1gi2CsprqfGmrybiUNoEnZuwjkO73EjmBK5l1xiIiwI+AZ4wx/+p6aCVwgX37AuA21/7zRKRNROYDC4GH6jMYdVMrShDxmz3qSYl72qZizLiKJCtncqn0lIKbOskWgZ9l7BLjuLTDhMp1xlHd1L4x4xDLeMgvSCJxIvB+4EkRedze9znga8DNInIR8DJwLoCdnHkz8DRWJvallfoHREbXM1aUQIa5GPvNIT6Wca11xlFXbSq4qRNsEYiUutnLLONmx4wDLGPfmHEtbupkNv0wxtxH8OXiaQHPuRq4uu6DKSwUoWKsKF6Gr5sa/MVPpLypw6B7U/tM3m6GRQKXt5Qo54kZN6MdZpBlXE1pUz0TuIb3te2gUTe1ogQyzMXYz01dxzrjQgJXhQ5chYULkjwJVXBTNyVmHCTGEXtTg7+oRrGME5TAFRtE7PCCWsaK4mXkiXE9O3BVaxkHiXUS8CttSrtjxs1wU7vH43ZTu75D35hxxASuXH9xX0ITuGKHpBJ+UaoojWH4iXFJXNPPTe1nGdcaM45IPBN7qqOstMk01zIeTMy4xE3tF+e1/z++ebjr/LYYF/qMJ6vOODZIWt3UiuLD8BNj9w89MGYckk1dy4RayU09HMS4rANXk2PGQe0wvWMZTAcuKDaPUTd1fUil1U2tKD4MPzF2C3CQm7puMWP3OQmpMx4GYuznpm6qZRxW2lQhgatiaZOLnq32+RO7UES8kJRmUyuKD8NPjE0FMS7pwDXIOuOy51SKGScY3w5czY4ZB7XDrFRnXIVlXBBjT8xYLePaUDe1ovgy/MTYbRm7F4hw8OvAVbc64wCGuga3EXhLiZouxt7SJneoocJ6xpE7cOEjxmoZD4qUZlMrih/DT4zdV93ZvvLHK1rGDXBTDwfL2K+0Kd3s0qYAy7hib+qodcZYi464z5+wph+xQ7OpFcWX4SfGbgvJu+YuYJU2edZVHfQSipXqjIdDzFhK314+VzlruaHjGUzMOGKdMQTHjH3d1MPge2406qZWFF+Gn1+tomVczwSuiHXDw8VN7S1tcn9WqSG+roscM65U2uQXanCL8Rb4/kmw9Un7Iacfubqpa0KbfiiKL8PQMnaL8cHyx31Lm9wx41pKmyqs2jQs3NRQFjNuRE12VMpixjX2pvb7btxC27uvKMQlz1UxrolUWrOpFcWH4SfG7qtuP8vYt7RpsE0/RoKb2qe0qZbM87oR1oGrgthW6tDl/h/o3+95brr8GO9jSjCidcaK4scwFGO3ZewTMy5J4AI2PgDdm12PD0JgAi3j4SDG3gSuXPMt40i9qf2yqSu4sd1Wb3+P/3PVTV0bIhozVhQfht/ska8yZvyTMwb/miOyA1e+uZagEBIzrtSb2m05t5U/7hba7i0Bz1UxrolUWrOpFcWHYWgZV8im9ittajTDIWZclsDV5JhxWDtM9+ftGzOu0BTELbT7XvV/rlrGtaFuakXxZfiJsfuqeyCgtKlSjDcyngl5WLfD9MZoY5DAZYKaflTRm9o329r1vZZ0cXMl/2nMuDYkpW5qRfFh+IlxxZixuLTYRzxriRk71lXbGP/Hh7rspxG4s5eNiYEYR2364Se2VSRwuUmFlERJuskJbQlB3dSK4svw86tVbIfps4Ri6QHVv+bc18NxF8AxH6j+uYnBJX7ORUwtZWB1G463tCnATT12VvlzK2VbB/0PhL1fdVFHQ9JDFx5SlAQx/GaQkoUifMTY7WoMWmKxagROuryG5yUIt5vaEeVmx4yjWMajJpY/tWJTkID/gTA39LBI0hsCNJtaUXwZBv5TD9VYxr5LLCq+uGO0BTFuolvWPZ58nhIruVLCnNvCzfhkU0d5nheNF0dD3dSK4svwE+Mga8nB3YErcL3jBjH3hMadu+G4spfdlnH7uCYNx+0290zujpV69Hn+z62mA1fJ80J+Ls102ScJTeBSFF+Gn5u6kmXszqYeyiv0K1+tzgqLGyWWsbNoQhr+9nkGn5Ve43ic1/VeVKVb4YqXobXT/7kV3dQBoqsx48GTblWPlKL4MPxmkBLLOGA940Kdcb1ixhHEqC1AGJKCu8mG2zJuaW/egEyAGKcy4Ra7+zuuJoErzBWtbupopFsg29/sUShK7BiGbmp3AleAm1pjxtVTUtoUgwQudxOSMjGuQhircVOHWcbqpo5Gug1yKsaK4mX4WcaVXM/uDlx1ixmPhPpS8beMm0XKLcae77wal3FYb+pZy2HKItj7Cqz/o1rG9SDdGhA+UpSRzfC2jH2pEDPWxg3+uEubnG5XzbQGU5nixdSgxDikN3XraDjn32HcbHu/ivGgSbdAzm81NUUZ2Qw/Ma60VmqJZaxZnZHxdVM38cKlRIx9YsZRCUvg8i6XqNnUgyejbmpF8WP4iXEly1ikOLkGNQVRfIibmzpjjSOfH5wY+15QOP2nbYF1RDnsvGoZR0MTuBTFl+EnxhWtXU3gqgnfph/NFGNb/ExucGIc+hr2eQprGGtp06BJt6plrCg+DD8xrmgZV0jg0pixP35NNpppDTril88OLmbsh/f9ebd+NLU1aIJIt2kCl6L4MPwu5ytmU7stY11XNToBHbiahSO4256BR//T/7FayXvEOJJlrG7qSGgCl6L4Mvwu5wdrGdcUMx4Bq9BIqvg24yTGj/8cHvmJ57FBCqPzP1QWMw55v+qmjoa6qRXFl+EnxhWt3aCF452HqxDjkeTSlhgmcAH0dQc/VitllrH9PrXpx+DJtNmJd1rJoChuhp8YmwAxTtklLM1cKCLJlPSCjkOdsf3ajRTjMstY3dSDxikly6qrWlHcDEMxDrjiztg9lN0x4yDhVvwps4ybXGcM0Lcv+LFacS7Sqsmm1gSuaDjtR9VVrSglDL8ZxOv+eteP4UN3uCbLSpax4ou7A1cs3NS2hdXf4/NYvbKpnWYfjihrzHjQFMRYM6oVxc3wm0G8lvHoKXDICUXvs3tCHWzM2IyAxC2HuC0UERozHqTLOMhNHfa/oW7qaBTEWN3UiuJm+IlxWc2p0+7Q6apUIYGrJkZCnNkngaupdcb2a/c2wE3tfX9RLjo0gSsa6qZWFF+Gnxh748BpV+KWdaP4mG9G50gQ1hoo6cCVK+5rFmGWse9KTB7mvTF4zWNvzDjKRYe6qaORUTe1ovgx/GaQwG5MjmXszgqu16pNI8BdHdfSpuzB4MfC+OCvgh/zuqmjWL0jqcxtMDiWsWZTK0oJwy+Byxsz9lrGJR24BlnaNKIm4Jh24PJjsOMqS+BSMa4bmsClKL4kX4xzA/CPs+CJm6z7FWPGlTpwKb6UuKntbRzqjP0YrDC2dlrbjon2+aKIcfJ/SkOCxowVxZfku6l791rlLb+5Apa+x8cydspS3DFjp87Yz03dqIEmHHFZxgU3bgzqjBvBsRdYF2rLL7RfyxHjkPerYhwNzaZWFF+SL8aFCTIgDuy1jMFlGWtLvsg4YmNM/N3UgyWdgdddUrwf6X3qVVwk1E2tKL4Mn8t5rwvVoSyz1lC3mPGIwtW1bLiLcdlracy4bmTUTa0ofiRfjAuuZk/ZjUOhJ7XLsqtXzHjmsdZ20ZmDO08ScD4zt2Uch/WMh4JCzDgka17d1NHQbGpF8SX5bmqvq9l7vyxmXMEyrsbCmboIvrRrZHRfcn9+cagzTg+lZRzhtVSMo6FuakXxJfkziCOohbV2g+qMncdNceKsR9OPkSDEgLqpK6Fu6khoNrWi+JJ8Mfa6qYMSuFpGFfdpaVP1jKQELi+R2mEm/6c0JGg2taL4Mnzc1N5WjQ5OAtf7boEnb4GxM+Hgbvu5g3RTjyTEbRnHoc44ZpaxinE0Mm3WNquWsaK4Sf4MUhYzzkPL6OJ9R0QmLoBTPmPd19KmGnDHjOOwnvEQXghEavrR+GHUioj8WES2ichTrn0TReT3IvKCvZ3geuxKEVknIs+JyOl1HYyzrni2t66nVZSkk3wxdrup93bBwP4IVpOWNlWN202dj0ECl1rG1XA9cIZn3xXAXcaYhcBd9n1EZDFwHrDEfs41InV0gWg2taL4EusZJBJuN/W3lsAj11s9hWcfH/wctYyrR0ZwAlfC22EaY1YBuzy7zwZusG/fAJzj2n+jMabPGLMeWAeE/JiqJJWyBFktY0UpIfkxY28CF1iT5wX/B30+a91aB3ie635ILWNfCmIzAuuMh2c29TRjzGYAY8xmEZlq758F/Nl1XJe9r35k2tUyVhQPyRdjx9Xsdjmn0tDSbv35EZpNnbhJdYiIm2U8lDHjKNnUw+b/xu+N+HY7EZGLgYsB5s6dG/0VMm2aTa0oHuLrW4tK3hYGdxOBipNnnZp+jCRKOnBpzLiMGLupA9gqIjMA7O02e38XMMd13Gxgk98JjDHXGWOWG2OWT5kyJforq2WsKGUkbgYpw89NXWmi1phx9ZTUGZvSfc0g5e05Dpzxz9ZfvSm8z2G1atNK4AL79gXAba7954lIm4jMBxYCD9X1lTVmrChlDB83tZvI2dR16MA1YvArbYqZZfz6jzToxTwrg4UeEz9E5L+BFcBkEekCvgx8DbhZRC4CXgbOBTDGrBWRm4GngSxwqTF+yRWDQC1jRSljGIixzzwR1TIOTPBSyvBbKGLExIwjCG2MLWNjzPkBD50WcPzVwNUNG1CmTS1jRfEQ3xkkKn4X7WXLJnqxJ9f+nvKHJh826CENS9ylTXGoMxaxsubdDV6aSYzFOHaoZawoZQxTy7iC1eRn6cx7I7zpCzDndfUZ17AjZm5qsDwgbWOsRi/NRhP/opNpg/4YfGeKEiOSfznvK8YRLeOSXSmY+3qdVIMoJHDl41FnDEUxjgP6fxMddVMrShnJF2M/N3XUmHHJc0bKUog1EreYMVjfc6u6qRNHpk3d1IriIfkziJ9lXClmnMpAuq10XzNXIEoCJR24YhAzBru5y6jKxw0JahlHJtOulrGieBgGYuxX2hQhZvyxB6p7zojHbwnFZotxBlo6mjsGB3VTRyfTBjldQlFR3CRfjB2XqZso3Zm8FpVaxuHE1U0dGzFO/k9pyFDLWFHKSP4MUlMCF+WTuFo24fglcDVbgFrahyaBy3mNcXOCj2n2Z5Ek0q0aM1YUD8OgtKmWDlyUW8bqpq5ADEubzvk+jJ4CT/x3Y19nxlI493pY+JaQg/RiLjKOZWyMXgQrik3yxdi36UeEt+VN8lI3dTju3tRxaPoBVinaULHkL8Mfb/ZnkSQy7XbzmGyEBj2KMjJI/gxSSztMKL8iV8s4HL+YsX5mRVSMo5OxKxnUVa0oBZI/g/i6qWu42lbLuAIuN7WTfFPL5zxcUXdrdApirElciuKQfDGuNZu67DkqxqG4e1P3bIVRk6KFA0YKKsbRcRq1+PWGV5QRSvLF2LfpRw0ioW7GcNxu6p5t0Dm9ueOJG/r/E5328da2d29Th6EocSL5M0gt7TD9UMs4HHcHru4t0Dm1qcOJHXFZPSoJtI+ztgf3NHUYihInku9n1JjxEOF2U2+DyQubOxw3599UnOCbwZu+AMsvbN7rJ42O8dZWLWNFKTAMxLiGJRT9UDdjOO6mHz0xs4wPP6O5r3/y3zX39ZOGc+GkYqwoBSIpkIicISLPicg6EbnC5/H3isga++9+EVla/6EG4JfAVUvtoibghON8Pgd3W32FNWas1EohZrynmaNQlFhRUYxFJA38O/BWYDFwvogs9hy2HjjFGHM08FXgunoPNJBaO3ABTD+qvmMZ1thi3LPN2sbJMlaSRWun5WlRy1hRCkSxjI8H1hljXjLG9AM3Ame7DzDG3G+M2W3f/TMwu77DDKHW3tQAf303nPwZ+45axqE4buoDu6ytE/dTlGpJpSxXtSZwKUqBKGI8C3jFdb/L3hfERcAdgxlUVfhmU0eMGadboNXuUa1u6nDcbmoouhoVpRbax6llrCguovhz/VTK+B4ociqWGJ8U8PjFwMUAc+fOjTjECgzGTW2Nyt5oAlcozufjxPlUjJXB0D5exVhRXERRoC7AvXbcbGCT9yARORr4IXC2MWan34mMMdcZY5YbY5ZPmTKllvGWkx9kAldBhNUyDsdrGTexlEhJPh3j4eCuZo9CUWJDFDF+GFgoIvNFpBU4D1jpPkBE5gL/C7zfGPN8/YcZQr2afqibOpwyN7WKsTIIxh8Cu15q9igUJTZUVC1jTFZEPg78FkgDPzbGrBWRj9iPXwt8CZgEXCPWpJ01xixv3LBd+CVwVeVy9vW4K17cYtwyCjKtzR2PkmwmHwYHdloJgaMmNns0itJ0IpmQxpjbgds9+6513f4w8OH6Di0ifjHjajC2GKtlXAGXGKtVrAwWp4Pbjhdg7uuaOxZFiQHJz1ryc1NXdwJ7q2IciuNtOLhbk7eUwTPpUGu7/ZnmjkNRYkLyxdgvgasa1DKOhqhlrNSRCfNg4gL44zeg/0CzR6MoTWcYiPEg3dQFVIzDsT+ffFYbfiiDJ5W2FtjY1wU7nmv2aBSl6SRfjOvmplZCcSfFqWWs1INxdq8Bp8Wqooxgki/GvtnUVVi5BTd18j+KhuL+TFWMlXrg9DdXMVaU4SDGWch01P58jRlHo8QyHt+0YSjDiIIYb23uOBQlBiRfjE0e2jo9O6sRVs2mjoZaxkqdaemAtrFqGSsKEeuMY00+Zy3Jtn97bc9Xyzga6qYu5/K1xY5kSm10ToX9KsaKknwxNjkfy7iqE9hbFeNQ3G5qzaa2GDfb+lNqZ/RUtYwVheHgps5nId1W+/PVMo6IWsZKA+icqjFjRWE4WMYHdkHHhNJ9h55WxQnUMo5E25ji7SoSuAYGBujq6qK3t7f+YxpmtLe3M3v2bFpaqlh1LOl0ToMX7272KBSl6SRbjHMDsO1peN0l8JL9g76qyjVS1TKOhtO+EKqyjLu6uhgzZgzz5s1D9DMOxBjDzp076erqYv78+c0eztDROQX69sLAQSuhS1FGKMl2U29/FnL9MH3pIE6ilnEk0q7rtirEuLe3l0mTJqkQV0BEmDRp0sjzIHROs7YaNx7Z7HkZbv87y8AaoSRbjLc8ZW1nHF37OdQyjk7LKGvbNraqp6kQR2NEfk6OGNdaDaEMD174PTx0HWx6DB7/eXFeHkEkW4wP7LC2Y2YM4iRqGUfmYw/AuddDKjn/Njt37mTZsmUsW7aM6dOnM2vWrML9/v7+0OeuXr2ayy67rOJrnHDCCfUa7shj9BRrq0lcI5v+Hmv7y49af8/+urnjaQLJjhn377e2raNrP0fBMh78cIY9E+ZZfwli0qRJPP744wBcddVVdHZ28ulPf7rweDabJZPx/xksX76c5cuXV3yN+++/vy5jHZE4lvE9X7PKnOa8trnjUZpDX7e13bnO2o7Ai7PkmDh+9HVbrtNUehAnUct4pPHBD36QT33qU5x66ql89rOf5aGHHuKEE07gmGOO4YQTTuC556xVhO655x7e/va3A5aQX3jhhaxYsYIFCxbw3e9+t3C+zs7OwvErVqzgXe96F4sWLeK9730vxr7Yu/3221m0aBEnnXQSl112WeG8Ix7HMt6yBm44C373Bdj1UnPHpAw9jhg7DBxszjiaSPItY8cqnn08bHq0+nNozHjI+Mr/reXpTfvqes7FM8fy5bOWVP28559/njvvvJN0Os2+fftYtWoVmUyGO++8k8997nP84he/KHvOs88+y9133013dzeHH344H/3oR8vKkB577DHWrl3LzJkzOfHEE/nTn/7E8uXLueSSS1i1ahXz58/n/PPPr/n9DjsyrfCX/wFPr4Tnfg33/5v197lNg/N4Kcmir6f0vlrGCcMtxhf9Dr60s4aTqGU8Ejn33HNJpy2Pyt69ezn33HM58sgjufzyy1m7dq3vc84880za2tqYPHkyU6dOZevW8gnj+OOPZ/bs2aRSKZYtW8aGDRt49tlnWbBgQaFkScXYw9LzrPJEN/d9uylDGVFsuA++ewzsr2XerDN9nov0fZuaM44mknDLuAda7WYUtVq2ahkPGbVYsI1i9Oii1fXFL36RU089lVtvvZUNGzawYsUK3+e0tRU7vaXTabLZbKRjzAjMDK2aqYuLt49+D6z6Bhz+Vph1rLXPGP2N1pM/fgPu/gfr9oZ7Yck5TR1OmZu6e3NzxrHnZZA0jJs15C+dcMu4Z/CurIw9eQ6mpaaSaPbu3cusWdaP7/rrr6/7+RctWsRLL73Ehg0bALjpppvq/hqJp9OOHS/5S3jr161e6D841XJZ73gBvj4f1v6yqUMcVjhCDFbjpGbT73FT73gecuUXuw3n20fBtxZXPq4BJFyM9w9ejE+6HE78JCy/sD5jUhLHZz7zGa688kpOPPFEcrlc3c/f0dHBNddcwxlnnMFJJ53EtGnTGDdO+3uX8bnN8I4fWguROIuR/O4L8KvLrdWx1txc+RzZvhHdOCISvbZLeP4pMG6uVdvbbPq6oXO6dbt9vFV3/vxvio93b4HeKrsrDoZ8fuhey0aa5UJbvny5Wb169eBO8r3jYeoiePd/1mdQSt155plnOOKII5o9jKbT09NDZ2cnxhguvfRSFi5cyOWXX152nN/nJSKPGGMq11g1kbr8nt3c9nF47Kel+ya+Bi6rkKT5zUUwbg58+Pf1G8tw495vwl1/D391M6y7Ex7+IXxgJcx/Y/PG9K9LYNpieOF38PqPwbO/siplLrkXtjwJ/3k2jJ4MF/4WxkxrzBgO7LI8MACXPQ4TG9OWNuj3PAws48Esn6goQ8MPfvADli1bxpIlS9i7dy+XXHJJ5SeNZN72DTjn+8X7x30Qdr0Iz94Omx4vPXbrWvjD1XDNG6xYY9dDI7KDUyR2vmgJMcCMZXDal61a7z9/P/RpDaevGybMh2MvgCPOgtP/0Wp3/Pwd8Oj10N8Nu9fDz99tfd8OxsAtF8FP3wH7AuLMT98Gq39sZewHWby9++Dfji3ev+3jkK+/lyyMYZDApeUPSvy5/PLLfS3huCIiZwDfAdLAD40xXxvSAbR0wFHvht0bYM7xMGURPHID3Ghnop/8GSu0NHaGNRlvf6b0+fterX6t6Rfvtlyh80+2tg2yjJrK87+1th+4rWhhLjoTHvsZbPgTvHw/LL8IRk2s/ty9e61WudUm2hljiW37WHjT1619uQErOXflZYCxvpPlF8IvPwY/eRt8+C6YfKhlNT91i/WcO78M77iu/Pw3f6B4+8x/tfKENj1u5SY43QRfvMsKhThsvA9efcT63xsikivGxthirJaxotQTEUkD/w68GegCHhaRlcaYoc30SWfg1M8V759/I2x+Au75R1j1dcuV+bEH4OCu4jFHnwdrboTvnwhz3wBv/Vpp17jcAKQy5YJhDPz0HOv2hHmWy/Kjf7K6guX64H8+BNOWwKF/AWtvtVrxvulL1kI1oybC2JkN+hAq8PKDVvLbhPn+Ivj876wV7ea+wVr7/YHvweTDYMGK4jFLz7cudK5/m3X/4R9bmexjZkBbpzXHto62Y/kTin+tnZZ3ct2d8OdroOtha3W3o94NR59rxaPTESTm4G4w+dK5PN0Ch70FnrLr/acdZSX3zVgGPzwNfn4u/L9/s74LBI79ADx6A7z2w5aArl8FT9wEi95W+lq//lTx9phpcPLfwZ5XihcpAJevhW8dab2vGcusWvghILlinOu3/rnUMlaUenM8sM4Y8xKAiNwInA3ULMZf/OVTzBzfwZlHzWDq2DbaMqnqF8Y4/Azrr60Tfvs5Kwv45g9YDSJecxq85auWGKz/o+Wufv4OKwZ5yAnQORX6D1gWUMcEmH4U7N4IY6bDqEmQda2W1b0VTM7KrHXz4l1w/3ctiy2Vgn+3W3emW61JO91qzUltY6yVzTrGW9uUZ5rt67ESlNrG2N0DxRJSSRVvg3Xfu79wO2W970d+Yh3bOc0SocmHQ+8e2P6c9fkcsGuI/3yNtR07y7II3cxeDpc/BRv/ZK2e9cyvYM1N5RnOYYyaBCd9yhLke/7R+kOsC5W2sdZ3lm6zPqN0i721P68XbCF0XyAAnP5PsLcLXnkQxs+x9k2cD+f9N9z0Xrj+TGvfkr+03Nrr7oSf/qX1mt1bLI14/L+sY/76DzD+EHj8Z/DSH63H/vAPcN93LKscYOFb4OxrrIubWcfBH/8ZVv2LFUsfNclyW+ez9vjbrO9wbxdMOMS6GEu1wIorYfSk6J+bi/gmcG283/oggsgPWFc/Z/wzvP4j9R+gUhc0gas64pDAJSLvAs4wxnzYvv9+4HXGmI8HPSfs9zyQy/Oxnz3K758uNklJCYxqzdDekqYtk6IlLbSkU/afJTyCpT8p122xb7fQz4V7v88bD9xJVlr40qR/YX16Ptm8YUJ2O8uzj/BU+2t524GVLBp4ijH5vYzJ7WNd6yIEw7TsZnZmptBq+ujM7yNHhn5pZX3rYTzYcTL7U528pec2pmU383zbEjZl5rCk7zFeajmMP3SeyZj8Pt6998e83LKA6dlXmZrdRIsZICdpOvIHGZ3vYZTpYVS+hzSlccp+WtiTnkiHOYiYvN1uyCCuP+s+hcet/dbtlOt8D3WcxFNtx3BY/9Mc1reWybmt5MiwsfU1dKfG0p0ay3+O/xizBjaSlxTrMwvZ1D2AMRQ++5Trc8X5nBHSDNBueukwBxllDtCZ30+n6aYz302n6abDHCQvaZ5tO4oXWxeRE6sb3ZTsZo7qfYRx2V2Mze+hI7+fDnOAjMmSNgNkyJExA6RNlhR59qbGs7ZtKb8ccz7GQN4Y8rYstZp+3nbgNn436u30porrXXfk93N0/2MYUqxuez1G0kzNbuad+39OyuTpSY3ld6PP5Njeh5iZ6+JHYy8lL8W2yR2mlzMOrGRSbjvbWmaxOz2Z59uPZG/aEtJpA12c2nM7nfm9zBlYT0f+ADnS5CRNxmRpMQP0pdrYnJnDtOyrjM3tIU2O/ov+yKRZC4J+Jvbn6/97jq8Yv/THYqJBEOlWK9Fj+pH1HZxSN1SMqyMmYnwucLpHjI83xnzCc9zFwMUAc+fOPW7jxo2B5zTGsKZrL89u2cfO/f0c6MtxoD/HwYEcA7l84a8/axjI5S05sucmZ4I2Bgz2RG3flnyOnIF0JkMmJaRTQkqEvDFkc4ZsPk8ub2KzPGVdR+HXCMVYYm4kODd3UmcrbZk0B/pz9GVzhc/VOWXZfSh83u59xphC/0L3UNwjcn/ubrEv7iu5A0DG/g4NJjQPL/Qxgh90jz9vitt6cN37j2Pq2PbQY4J+z/F1Uy84BRbc1exRKAln586dnHbaaQBs2bKFdDrNlClWg4mHHnqI1tbweNA999xDa2vrSFsmsQuY47o/GyjrT2iMuQ64DqyL67ATighL54xn6ZzxdRymogwf4ivGilIHKi2hWIl77rmHzs7OkSbGDwMLRWQ+8CpwHvBXzR2Sogxvkl1nrCg18Mgjj3DKKadw3HHHcfrpp7N5s1Wf+N3vfpfFixdz9NFHc95557FhwwauvfZavvWtb7Fs2TLuvffeJo98aDDGZIGPA78FngFuNsb4r56hKEpdUMtYGTruuMKqC6wn04+yylciYozhE5/4BLfddhtTpkzhpptu4vOf/zw//vGP+drXvsb69etpa2tjz549jB8/no985CNVW9PDAWPM7cDtzR6HoowUVIyVEUVfXx9PPfUUb37zmwHI5XLMmDEDgKOPPpr3vve9nHPOOZxzzjlNHKWiKCMNFWNl6KjCgm0UxhiWLFnCAw88UPbYr3/9a1atWsXKlSv56le/GriusaIoSr3RmLEyomhra2P79u0FMR4YGGDt2rXk83leeeUVTj31VL7+9a+zZ88eenp6GDNmDN3d3RXOqiiKMjhUjJURRSqV4pZbbuGzn/0sS5cuZdmyZdx///3kcjne9773cdRRR3HMMcdw+eWXM378eM466yxuvfXWEZXApSjK0KNuamXEcNVVVxVur1q1quzx++67r2zfYYcdxpo1axo5LEVRFLWMFUVRFKXZqBgriqIoSpNRMVYURVGUJqNirDScZi1GkjT0c1KUkYuKsdJQ2tvb2blzpwpNBYwx7Ny5k/b28BVfFEUZnmg2tdJQZs+eTVdXF9u3b2/2UGJPe3s7s2fPbvYwFEVpAirGSkNpaWlh/vz5zR6GoihKrFE3taIoiqI0GRVjRVEURWkyKsaKoiiK0mSkWVmuIrId2FjhsMnAjiEYThTiMhYdRzlxGUujxnGIMWZKA85bNxL2e9ZxlBOXsYyEcfj+npsmxlEQkdXGmOXNHgfEZyw6jnLiMpa4jCOuxOXz0XGUE5exjORxqJtaURRFUZqMirGiKIqiNJm4i/F1zR6Ai7iMRcdRTlzGEpdxxJW4fD46jnLiMpYRO45Yx4wVRVEUZSQQd8tYURRFUYY9sRVjETlDRJ4TkXUicsUQv/YGEXlSRB4XkdX2voki8nsRecHeTmjQa/9YRLaJyFOufYGvLSJX2p/RcyJyeoPHcZWIvGp/Lo+LyNuGYBxzRORuEXlGRNaKyCft/UP6mYSMY8g/k6TRzN+y/fpN+T3rb7lsHLH4LVcYS/N+z8aY2P0BaeBFYAHQCjwBLB7C198ATPbs+zpwhX37CuCfG/TaJwPHAk9Vem1gsf3ZtAHz7c8s3cBxXAV82ufYRo5jBnCsfXsM8Lz9ekP6mYSMY8g/kyT9Nfu3bI+hKb9n/S2XnTsWv+UKY2na7zmulvHxwDpjzEvGmH7gRuDsJo/pbOAG+/YNwDmNeBFjzCpgV8TXPhu40RjTZ4xZD6zD+uwaNY4gGjmOzcaYR+3b3cAzwCyG+DMJGUcQDftMEkYcf8swBL9n/S2XjSMWv+UKYwmi4b/nuIrxLOAV1/0uwj+oemOA34nIIyJysb1vmjFmM1hfJDB1CMcT9NrN+Jw+LiJrbNeX404aknGIyDzgGOBBmviZeMYBTfxMEkAcPoc4/Z71t0x8fss+Y4EmfS5xFWPx2TeUad8nGmOOBd4KXCoiJw/ha1fDUH9O3wdeAywDNgPfHKpxiEgn8Avgb4wx+8IObeRYfMbRtM8kIcThc0jC71l/yz6HNmEsTftc4irGXcAc1/3ZwKahenFjzCZ7uw24FcsdsVVEZgDY221DNZ6Q1x7Sz8kYs9UYkzPG5IEfUHTTNHQcItKC9YP5mTHmf+3dQ/6Z+I2jWZ9Jgmj65xCz37P+lmPwWw4aSzN/z3EV44eBhSIyX0RagfOAlUPxwiIyWkTGOLeBtwBP2a9/gX3YBcBtQzEem6DXXgmcJyJtIjIfWAg81KhBOD8Ym7/E+lwaOg4REeBHwDPGmH91PTSkn0nQOJrxmSSMpv2WIZa/Z/0tN/m3HDaWpv6e65kNVs8/4G1YGW4vAp8fwtddgJU19wSw1nltYBJwF/CCvZ3YoNf/byz3yADW1dhFYa8NfN7+jJ4D3trgcfwUeBJYY/9zzhiCcZyE5Q5aAzxu/71tqD+TkHEM+WeStL9m/Zbt127a71l/y2XjiMVvucJYmvZ71g5ciqIoitJk4uqmVhRFUZQRg4qxoiiKojQZFWNFURRFaTIqxoqiKIrSZFSMFUVRFKXJqBgriqIoSpNRMVYURVGUJqNirCiKoihN5v8DBQITBRMem/UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training')\n",
    "plt.plot(epochs_range, val_acc, label='Test')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training')\n",
    "plt.plot(epochs_range, val_loss, label='Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
